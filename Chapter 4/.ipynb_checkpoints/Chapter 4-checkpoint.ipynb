{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1\n",
    "What Linear Regression training algorithm can you use if you have a training set with millions of features?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I would use gradient descent to minimize a cost function instead of using the Normal Equation, since the Normal Equation computes the inverse of $X^\\intercal * X$, which is an n x n matrix where n is the number of features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2\n",
    "Suppose the features in your training set have very different scales. What algorithms might suffer from this, and how? What can you do about it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient descent will suffer from this. It will always obtain the best solution regardless of the difference in scale (since the cost function of Linear Regression is convex), but if the features have different scales it will take longer to reach that minimum. The best solution would be to use one of Scikit-Learn's scalers (e.g. StandardScaler) to scale the different features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3\n",
    "Can Gradient Descent get stuck in a local minimum when training a Logistic Regression model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No, it can't. This is because the Logistic Regression cost function is also convex, so Gradient descent is guaranteed to arrive to the best solution (global minimum) given that the learning rate isn't too high and we wait for enough iterations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4\n",
    "Do all Gradient Descent algorithms lead to the same model provided you let them run long enough?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No, they don't. While it's true that all Gradient Descent algorithms will finish around the global minimum, the Stochastic and Mini-batch variants will bounce around the minimum, while Batch Gradient descent will stop at the exact global minimum given enough time. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5\n",
    "Suppose you use Batch Gradient Descent and you plot the validation error at every epoch. If you notice that the validation error consistently goes up, what is likely going on? How can you fix this?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the validation error is going up that means that Gradient Descent is diverging from the global minimum. This is most likely caused by setting a high learning rate, so the solution would be to lower the learning rate until the validation error starts going down."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 6\n",
    "Is it a good idea to stop Mini-batch Gradient Descent immediately when the validation error goes up?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No, it is not. As said in exercise 4, Mini-batch gradient descent makes bounces when arriving to the solution (because it is evaluating part of the samples before making the next step, and not all the samples like Btach Gradient Descent does), so sometimes the validation error may go up."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 7\n",
    "Which Gradient Descent algorithm (among those we discussed) will reach the vicinity of the optimal solution the fastest? Which will actually converge? How can you make the others converge as well?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usually the Stochastic and Mini-Batch (with small batch size) variants will reach the vicinity of the optimal solution the fastest. However, Batch gradient descent will be the one that converges to the solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 8\n",
    "Suppose you are using Polynomial Regression. You plot the learning curves and you notice that there is a large gap between the training error and the validation error. What is happening? What are three ways to solve this?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means that we are overfitting the data. The three posible ways to solve this are:\n",
    "* Getting more training data.\n",
    "* Using a regularized Linear Model (e.g. Ridge Regression).\n",
    "* Removing some of the features used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 9\n",
    "Suppose you are using Ridge Regression and you notice that the training error and the validation error are almost equal and fairly high. Would you say that the model suffers from high bias or high variance? Should you increase the regularization hyperparameter or reduce it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model suffers from high bias (underfitting). We should reduce the regularization hyperparameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 10\n",
    "Why would you want to use:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge Regression instead of plain Linear Regression (i.e., without any regularization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One should almost use Ridge Regression instead of plain Linear Regression, since you can always tune the regularization hyperparameter to fit your needs. Furthermore, it is almost useful to perform regularization, even if it is just a little."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lasso instead of Ridge Regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lasso tends to eliminate the weights of useless features (setting them to 0), so it should be preferred if you think that only some of your features will be actually useful to train the model. Lasso can also help you detecting the useless features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elastic net instead of Lasso?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Elastic Ner is a mix of both Ridge Regression and Lasso. It should be almost always preferred to Lasso, as it provides you a lot more flexibility and tuning to fit your needs. In the worst case it can perform exactly like Lasso, but you can tune it to behave more like Ridge Regression if needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Exercise 11\n",
    "Suppose you want to classify pictures as outdoor/indoor and daytime/nighttime. Should you implement two Logistic Regression classifiers or one Softmax Regression classifier?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should implement two Logistic Regression classifiers, one to classify the picture as outdoor/indoor and other to classify it as daytime/nighttime. Softmax Regression should be used if you have to choose a class from many of them (more than 2), but it is not a multioutput classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 12\n",
    "Implement Batch Gradient Descent with early stopping for Softmax Regression (without using Scikit-Learn)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "class CustomSoftmaxRegressor():\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.weights = []\n",
    "        self.Y = [[]]\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        m = np.shape(X)[1]\n",
    "        n = np.shape(X)[0]\n",
    "        \n",
    "        self._compute_classes_matrix(y)\n",
    "        self.weights = np.zeros((self.num_classes, m))\n",
    "        \n",
    "        delta = np.zeros((self.num_classes,))\n",
    "        error = sys.maxsize\n",
    "        num_iter = 1\n",
    "        while (error > 1e-5 and num_iter < 500):\n",
    "            for i in range(self.num_classes):\n",
    "                delta[i] = np.sum((self._softmax_proba(X, i) - self.Y[:, i]) * np.transpose(X)) / m\n",
    "                self.weights[i] -= delta[i]\n",
    "            print(\"Iter: {0} - Weights: {1} - Cost: {2}\".format(num_iter, self.weights, self._compute_cost(X,self.Y)))\n",
    "            \n",
    "            # error = self._compute_cost(X, self.Y)\n",
    "            num_iter += 1\n",
    "        \n",
    "    def predict(self, X):\n",
    "        prob_matrix = np.zeros((np.shape(X)[0], self.num_classes))\n",
    "        for i in range(self.num_classes):\n",
    "            prob_matrix[:, i] = self._softmax_score(X, i)\n",
    "        print(prob_matrix)\n",
    "        return np.max(X, axis=1)\n",
    "        \n",
    "    def _softmax_proba(self, X, k):\n",
    "        \"\"\" \n",
    "        Computes the probability that each sample in the\n",
    "        data matrix X has of belonging to class k.\n",
    "        \"\"\"\n",
    "        total_score = np.zeros(np.shape(X)[0])\n",
    "        k_score = np.exp(self._softmax_score(X, k))\n",
    "        for i in range(self.num_classes):\n",
    "            total_score += np.exp(self._softmax_score(X, i))\n",
    "        return k_score / total_score\n",
    "    \n",
    "    def _softmax_score(self, X, k):\n",
    "        assert 0 <= k < self.num_classes\n",
    "        return np.dot(self.weights[k], np.transpose(X))\n",
    "    \n",
    "    def _compute_classes_matrix(self, y):\n",
    "        _, counts = np.unique(y, return_counts=True)\n",
    "        m = np.size(y)\n",
    "        self.num_classes = np.size(counts)\n",
    "        self.Y = np.zeros((m, self.num_classes))\n",
    "        for i in range(m):\n",
    "            self.Y[i, :] = [1 if y[i] == k else 0 for k in range(self.num_classes)]\n",
    "\n",
    "    def _compute_cost(self, X, Y):\n",
    "        m = np.shape(X)[0]\n",
    "        cost = 0\n",
    "        for i in range(self.num_classes):\n",
    "            cost += np.sum(np.dot(Y[:, i] * np.log(self._softmax_proba(X, i)), X)) / m\n",
    "        return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E\n",
      "======================================================================\n",
      "ERROR: /Users/alejandro/Library/Jupyter/runtime/kernel-9de1f772-a646-4271-8e40-1656a000e9e6 (unittest.loader._FailedTest)\n",
      "----------------------------------------------------------------------\n",
      "AttributeError: module '__main__' has no attribute '/Users/alejandro/Library/Jupyter/runtime/kernel-9de1f772-a646-4271-8e40-1656a000e9e6'\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 0.001s\n",
      "\n",
      "FAILED (errors=1)\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "True",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alejandro/anaconda/envs/ml_lab/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2918: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "import unittest\n",
    "\n",
    "class TestSoftmaxRegression(unittest.TestCase):\n",
    "    \n",
    "    def setUp():\n",
    "        self.clf = CustomSoftmaxRegressor()\n",
    "    \n",
    "    def test_softmax_score():\n",
    "        pass\n",
    "\n",
    "    def test_compute_cost():\n",
    "        pass\n",
    "\n",
    "    def test_softmax_proba():\n",
    "        pass\n",
    "    \n",
    "unittest.main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.95228967, 0.98618932])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.random.rand(2, 3)\n",
    "b = np.random.rand(2, 3)\n",
    "np.max(a, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.95228967, 0.65806659, 0.45680161],\n",
       "       [0.9181056 , 0.98618932, 0.98167172]])"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "nbTranslate": {
   "displayLangs": [
    "en",
    "es"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "es",
   "targetLang": "en",
   "useGoogleTranslate": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
