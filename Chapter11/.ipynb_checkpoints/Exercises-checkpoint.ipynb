{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 11: Training Deep Neural Nets "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "source": [
    "## Exercise 1\n",
    "Is it okay to initialize all the weights to the same value as long as that value is selected randomly using He initialization?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "hidden"
   },
   "source": [
    "No. If you initiaze all the weights with the same value, even if it is obtained using He initialization, you won't break the simmetry of each layer. The neural network will behave as if it had just one neuron per layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "source": [
    "## Exercise 2\n",
    "Is it okay to initialize the bias terms to 0?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "hidden"
   },
   "source": [
    "Yes, it is ok."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "source": [
    "## Exercise 3\n",
    "Name three advantages of the ELU activation function over ReLU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "hidden"
   },
   "source": [
    "* It has a non-zero gradient when z < 0, which avoid the dying units problem.\n",
    "* The function is smooth everywhere, which speeds up gradient descent, since it does not bounce so much left and right of z = 0\n",
    "* It takes negative values when z < 0, which allows the unit to have an average output closer to 0. This helps alleviate the vanishing gradients problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "source": [
    "## Exercise 4\n",
    "In which cases would you want to use each of the following activation functions: ELU, leaky ReLU (and its variants), ReLU, tanh, logistic, and softmax?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "hidden"
   },
   "source": [
    "* ELU: Almost always. Only drawback is that the ELU function is quite slow to compute.\n",
    "* Leaky ReLU: To avoid the dying units problem that ReLU has.\n",
    "* ReLU: Need speed. Good default, but ELU and Leaky ReLU can be better.\n",
    "* Tanh: If you need to output a number between 1 and -1. Rarely used.\n",
    "* Logistic: To estimate probabilities. Also rarely used.\n",
    "* Softmax: You need to output probabilities of mutually exclusive classes. Usually used in the output layer for classification tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "source": [
    "## Exercise 5\n",
    "What may happen if you set the momentum hyperparameter too close to 1 (e.g. 0.99999) when using a MomentumOptimizer?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "hidden"
   },
   "source": [
    "If you set the momentum hyperparameter too close to 1 the system will have almost no friction, so the gradient steps can get too high and the system may not converge to a good solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "source": [
    "## Exercise 6\n",
    "Name three ways you can produce a sparse model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "hidden"
   },
   "source": [
    "* Setting to 0 all the weights with really small values.\n",
    "* Using a high $l1$ regularization during training, which will force the optimizer to zero out as many weights as it can.\n",
    "* Applying other techniques, such as Follow The Regularized Leader."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "source": [
    "## Exercise 7\n",
    "Does dropout slow down training? Does it slow down inference (i.e. making predictions on new instances)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "hidden"
   },
   "source": [
    "Dropout will slow training a bit, but inference will be the same (you only have to multiply the output of each neuron by the keep ratio)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "source": [
    "## Exercise 8\n",
    "Deep Learning\n",
    "* a) Build a DNN with five hidden layers of 100 neurons each, He initialization, and the ELU activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "he_init = tf.contrib.layers.variance_scaling_initializer()\n",
    "\n",
    "def build_dnn_ex8a(X):\n",
    "    hidden_1 = tf.layers.dense(X, 100, activation=tf.nn.elu, kernel_initializer=he_init, name=\"hidden1\")\n",
    "    hidden_2 = tf.layers.dense(hidden_1, 100, activation=tf.nn.elu, kernel_initializer=he_init, name=\"hidden2\")\n",
    "    hidden_3 = tf.layers.dense(hidden_2, 100, activation=tf.nn.elu, kernel_initializer=he_init, name=\"hidden3\")\n",
    "    hidden_4 = tf.layers.dense(hidden_3, 100, activation=tf.nn.elu, kernel_initializer=he_init, name=\"hidden4\")\n",
    "    hidden_5 = tf.layers.dense(hidden_4, 100, activation=tf.nn.elu, kernel_initializer=he_init, name=\"hidden5\")\n",
    "    return hidden_5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "source": [
    "* b) Using Adam optimization and early stopping, try training it on MNIST but only on digits 0 to 4, as we will use transfer learning for digits 5 to 9 in the next exercise. You will need a softmax output layer with five neurons, and as always make sure to save checkpoints at regular intervals and save the final model so you can reuse it later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true,
    "solution2": "hidden"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-6-e761c88f755e>:7: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "0\tValidation loss: 0.095365\tBest loss: 0.095365\tAccuracy: 96.951%\n",
      "1\tValidation loss: 0.067606\tBest loss: 0.067606\tAccuracy: 97.615%\n",
      "2\tValidation loss: 0.051031\tBest loss: 0.051031\tAccuracy: 98.319%\n",
      "3\tValidation loss: 0.049486\tBest loss: 0.049486\tAccuracy: 98.593%\n",
      "4\tValidation loss: 0.040611\tBest loss: 0.040611\tAccuracy: 98.788%\n",
      "5\tValidation loss: 0.032139\tBest loss: 0.032139\tAccuracy: 99.218%\n",
      "6\tValidation loss: 0.042180\tBest loss: 0.032139\tAccuracy: 98.827%\n",
      "7\tValidation loss: 0.033423\tBest loss: 0.032139\tAccuracy: 99.023%\n",
      "8\tValidation loss: 0.047029\tBest loss: 0.032139\tAccuracy: 98.944%\n",
      "9\tValidation loss: 0.043115\tBest loss: 0.032139\tAccuracy: 99.101%\n",
      "10\tValidation loss: 0.044584\tBest loss: 0.032139\tAccuracy: 99.140%\n",
      "11\tValidation loss: 0.035370\tBest loss: 0.032139\tAccuracy: 99.140%\n",
      "12\tValidation loss: 0.058927\tBest loss: 0.032139\tAccuracy: 98.866%\n",
      "13\tValidation loss: 0.053299\tBest loss: 0.032139\tAccuracy: 98.749%\n",
      "14\tValidation loss: 0.026517\tBest loss: 0.026517\tAccuracy: 99.453%\n",
      "15\tValidation loss: 0.047550\tBest loss: 0.026517\tAccuracy: 98.984%\n",
      "16\tValidation loss: 0.037035\tBest loss: 0.026517\tAccuracy: 99.062%\n",
      "17\tValidation loss: 0.037683\tBest loss: 0.026517\tAccuracy: 99.257%\n",
      "18\tValidation loss: 0.044062\tBest loss: 0.026517\tAccuracy: 99.140%\n",
      "19\tValidation loss: 0.039112\tBest loss: 0.026517\tAccuracy: 99.101%\n",
      "20\tValidation loss: 0.053575\tBest loss: 0.026517\tAccuracy: 98.984%\n",
      "21\tValidation loss: 0.062283\tBest loss: 0.026517\tAccuracy: 99.023%\n",
      "22\tValidation loss: 0.045043\tBest loss: 0.026517\tAccuracy: 99.218%\n",
      "23\tValidation loss: 0.043189\tBest loss: 0.026517\tAccuracy: 99.296%\n",
      "24\tValidation loss: 0.034686\tBest loss: 0.026517\tAccuracy: 99.335%\n",
      "25\tValidation loss: 0.035767\tBest loss: 0.026517\tAccuracy: 99.257%\n",
      "26\tValidation loss: 0.032080\tBest loss: 0.026517\tAccuracy: 99.414%\n",
      "27\tValidation loss: 0.036964\tBest loss: 0.026517\tAccuracy: 99.375%\n",
      "28\tValidation loss: 0.054790\tBest loss: 0.026517\tAccuracy: 99.101%\n",
      "29\tValidation loss: 0.089145\tBest loss: 0.026517\tAccuracy: 98.554%\n",
      "30\tValidation loss: 0.055558\tBest loss: 0.026517\tAccuracy: 99.179%\n",
      "31\tValidation loss: 0.063581\tBest loss: 0.026517\tAccuracy: 98.944%\n",
      "32\tValidation loss: 0.048146\tBest loss: 0.026517\tAccuracy: 99.179%\n",
      "33\tValidation loss: 0.048376\tBest loss: 0.026517\tAccuracy: 99.257%\n",
      "No progress after 34 epochs. Stopping...\n",
      "INFO:tensorflow:Restoring parameters from ./mnist_digits_0-4.ckpt\n",
      "Final test accuracy: 99.397%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "\n",
    "class MnistData():\n",
    "    def __init__(self, min_digit=0, max_digit=9):\n",
    "        mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "        digit_filter = np.vectorize(lambda t: t >= min_digit and t <= max_digit)\n",
    "        train_idx = digit_filter(np.argmax(mnist.train.labels, axis=1))\n",
    "        validation_idx = digit_filter(np.argmax(mnist.validation.labels, axis=1))\n",
    "        test_idx = digit_filter(np.argmax(mnist.test.labels, axis=1))\n",
    "        self.train_images = mnist.train.images[train_idx]\n",
    "        self.train_labels = np.argmax(mnist.train.labels[train_idx, min_digit:max_digit+1], axis=1)\n",
    "        self.validation_images = mnist.validation.images[validation_idx]\n",
    "        self.validation_labels = np.argmax(mnist.validation.labels[validation_idx, min_digit:max_digit+1], axis=1)\n",
    "        self.test_images = mnist.test.images[test_idx]\n",
    "        self.test_labels = np.argmax(mnist.test.labels[test_idx, min_digit:max_digit+1], axis=1)\n",
    "        \n",
    "def fetch_batch(batch_size, batch_idx, X, y):\n",
    "    start = batch_idx * batch_size\n",
    "    end = (batch_idx + 1) * batch_size\n",
    "    batch_x = X[start:end]\n",
    "    batch_y = y[start:end]\n",
    "    return batch_x, batch_y\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# build dnn\n",
    "mnist = MnistData(min_digit=0, max_digit=4)\n",
    "num_samples = np.shape(mnist.train_images)[0]\n",
    "num_classes = np.shape(np.unique(mnist.train_labels))[0]\n",
    "num_features = np.shape(mnist.train_images)[1]\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=[None, num_features], name=\"x_input\")\n",
    "y = tf.placeholder(tf.int64, shape=[None], name=\"y_input\")\n",
    "dnn = build_dnn_ex8a(X)\n",
    "output = tf.layers.dense(dnn, num_classes, activation=None, kernel_initializer=he_init, name=\"logits\")\n",
    "\n",
    "# training  \n",
    "with tf.name_scope('loss'):\n",
    "    loss_op = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=output))\n",
    "\n",
    "with tf.name_scope('train'):\n",
    "    optimizer = tf.train.AdamOptimizer()\n",
    "    training_op = optimizer.minimize(loss_op)\n",
    "\n",
    "with tf.name_scope('accuracy'):\n",
    "    softmax = tf.nn.softmax(output)\n",
    "    correct = tf.equal(tf.argmax(softmax, axis=1), y)\n",
    "    accuracy_op = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "    \n",
    "saver = tf.train.Saver()\n",
    "initializer = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(initializer)\n",
    "\n",
    "    num_epochs = 1000\n",
    "    batch_size = 100\n",
    "    num_batches = num_samples // batch_size\n",
    "\n",
    "    MAX_CHECKS_NO_PROGRESS = 20\n",
    "    checks_no_progress = 0\n",
    "    best_loss = np.inf\n",
    "    for epoch in range(num_epochs):\n",
    "        for batch in range(num_batches):\n",
    "            # training step\n",
    "            X_batch, y_batch = fetch_batch(batch_size, batch, mnist.train_images, mnist.train_labels)\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "\n",
    "        loss, acc = sess.run([loss_op, accuracy_op], feed_dict={X: mnist.validation_images, y: mnist.validation_labels})\n",
    "        if loss < best_loss:\n",
    "            best_loss = loss\n",
    "            checks_no_progress = 0\n",
    "            saver.save(sess, \"./mnist_digits_0-4.ckpt\")\n",
    "        else:\n",
    "            checks_no_progress += 1\n",
    "            if checks_no_progress >= MAX_CHECKS_NO_PROGRESS:\n",
    "                print(\"No progress after {} epochs. Stopping...\".format(epoch))\n",
    "                break\n",
    "        print(\"{}\\tValidation loss: {:.6f}\\tBest loss: {:.6f}\\tAccuracy: {:.3f}%\".format(\n",
    "            epoch, loss, best_loss, acc * 100))\n",
    "\n",
    "    saver.restore(sess, \"./mnist_digits_0-4.ckpt\")\n",
    "    acc_test = accuracy_op.eval(feed_dict={X: mnist.test_images, y: mnist.test_labels})\n",
    "    print(\"Final test accuracy: {:.3f}%\".format(acc_test * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "shown",
    "solution2_first": true
   },
   "source": [
    "* c) Tune the hyperparameters using cross-validation and see what precision you can achieve."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "shown"
   },
   "source": [
    "First of all we will move most of the code from before to a custom class which will hold all the hyperparameters that can be tweaked. After doing that, we will be able to use the RandomizedCV class from scikitlearn in order to obtain easily the best hyperparameters for our DNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "shown"
   },
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.exceptions import NotFittedError\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class DNNClassifier(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, hidden_layers=5, num_neurons=100, optimizer=tf.train.AdamOptimizer,\n",
    "                 batch_size=50, learning_rate=1e-4, activation=tf.nn.elu, initializer=he_init,\n",
    "                 batch_norm_momentum=None, dropout_rate=None, tensorboard_logdir=None, random_seed=42):\n",
    "        self.hidden_layers = hidden_layers\n",
    "        self.num_neurons = num_neurons\n",
    "        self.optimizer = optimizer\n",
    "        self.batch_size = batch_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.activation = activation\n",
    "        self.initializer = initializer\n",
    "        self.batch_norm_momentum = batch_norm_momentum\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.tensorboard_logdir = None\n",
    "        self.random_seed = random_seed\n",
    "        self.session = None\n",
    "    \n",
    "    def fit(self, X, y, num_epochs=1000):\n",
    "        X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.25, random_state=self.random_seed)\n",
    "        \n",
    "        num_features = np.shape(X)[1]\n",
    "        classes = np.unique(y)\n",
    "        num_classes = np.shape(classes)[0]\n",
    "        \n",
    "        self._graph = tf.Graph()\n",
    "        with self._graph.as_default():\n",
    "            self._build_graph(num_features, num_classes)\n",
    "        self.session = tf.Session(graph=self._graph)\n",
    "        self.session.run(self._init)\n",
    "\n",
    "        num_samples = np.shape(X_train)[0]\n",
    "        num_batches = num_samples // self.batch_size\n",
    "        MAX_CHECKS_NO_PROGRESS = 20\n",
    "        checks_no_progress = 0\n",
    "        best_loss = np.inf\n",
    "        with self.session.as_default() as sess:\n",
    "            for epoch in range(num_epochs):\n",
    "                for batch in range(num_batches):\n",
    "                    # training step\n",
    "                    X_batch, y_batch = fetch_batch(batch_size, batch, X_train, y_train)\n",
    "                    sess.run(self._training_op, feed_dict={self._X: X_batch, self._y: y_batch})\n",
    "                    if self.tensorboard_logdir is not None and batch_idx % 3 == 0:\n",
    "                        step = epoch * num_batches + batch\n",
    "                        s = self.session.run(self.summaries, feed_dict={self._X: X_batch,\n",
    "                                                                        self._y: y_batch})\n",
    "                        self.writer.add_summary(s, step)\n",
    "\n",
    "                loss, acc = sess.run([self._loss_op, self._accuracy_op], feed_dict={self._X: X_val, self._y: y_val})\n",
    "                if loss < best_loss:\n",
    "                    best_loss = loss\n",
    "                    checks_no_progress = 0\n",
    "                else:\n",
    "                    checks_no_progress += 1\n",
    "                    if checks_no_progress >= MAX_CHECKS_NO_PROGRESS:\n",
    "                        print(\"No progress after {} epochs. Stopping...\".format(epoch))\n",
    "                        break\n",
    "                print(\"{}\\tValidation loss: {:.6f}\\tBest loss: {:.6f}\\tAccuracy: {:.3f}%\".format(\n",
    "                    epoch, loss, best_loss, acc * 100))\n",
    "    \n",
    "    def predict(self, X):\n",
    "        if self.session is None:\n",
    "            raise NotFittedError()\n",
    "        else:\n",
    "            probabilities = self._predict_proba(X)\n",
    "            return np.argmax(probabilities, axis=1)\n",
    "\n",
    "    \n",
    "    def _predict_proba(self, X):\n",
    "        with self.session.as_default() as sess:\n",
    "            return self._Y_proba.eval(feed_dict={self._X: X})\n",
    "    \n",
    "    def _dnn(self, inputs):\n",
    "        for layer in range(self.hidden_layers):\n",
    "            if self.dropout_rate:\n",
    "                inputs = tf.layers.dropout(inputs, self.dropout_rate, training=self._training, seed=self.random_seed)\n",
    "            \n",
    "            inputs = tf.layers.dense(inputs, self.num_neurons, \n",
    "                                     kernel_initializer=self.initializer, name=\"hidden%d\" % (layer + 1))\n",
    "            if self.batch_norm_momentum:\n",
    "                inputs = tf.layers.batch_normalization(inputs, momentum=self.batch_norm_momentum)\n",
    "                \n",
    "            inputs = self.activation(inputs, name=\"hidden%d_out\" % (layer + 1))\n",
    "            \n",
    "        return inputs\n",
    "    \n",
    "    def _build_graph(self, num_inputs, num_outputs):\n",
    "        if self.random_seed is not None:\n",
    "            tf.set_random_seed(self.random_seed)\n",
    "            np.random.seed(self.random_seed)\n",
    "\n",
    "        X = tf.placeholder(tf.float32, shape=[None, num_inputs], name=\"x_input\")\n",
    "        y = tf.placeholder(tf.int64, shape=[None], name=\"y_input\")\n",
    "        if self.batch_norm_momentum or self.dropout_rate:\n",
    "            self._training = tf.placeholder_with_default(False, shape=(), name='training')\n",
    "        else:\n",
    "            self._training = None\n",
    "        \n",
    "        dnn = self._dnn(X)\n",
    "        logits = tf.layers.dense(dnn, num_outputs, activation=None, \n",
    "                                      kernel_initializer=self.initializer, name=\"logits\")\n",
    "        with tf.name_scope('loss'):\n",
    "            loss_op = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits), name=\"loss\")\n",
    "\n",
    "        with tf.name_scope('train'):\n",
    "            optimizer = self.optimizer(self.learning_rate)\n",
    "            training_op = optimizer.minimize(loss_op, name=\"training\")\n",
    "\n",
    "        with tf.name_scope('accuracy'):\n",
    "            y_proba = tf.nn.softmax(logits, name=\"y_proba\")\n",
    "            correct = tf.equal(tf.argmax(y_proba, axis=1), y)\n",
    "            accuracy_op = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")\n",
    "            \n",
    "        if self.tensorboard_logdir is not None:\n",
    "            now = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
    "            log_dir = os.path.join(self.tensorboard_logdir, \"run-{}\".format(now))\n",
    "            self.writer = tf.summary.FileWriter(log_dir)\n",
    "            tf.summary.scalar('loss', loss_op)\n",
    "            tf.summary.scalar('accuracy', accuracy_op)\n",
    "            self.summaries = tf.summary.merge_all()\n",
    "        \n",
    "        saver = tf.train.Saver()\n",
    "        init = tf.global_variables_initializer()\n",
    "        \n",
    "        # Make the important operations available\n",
    "        self._X, self._y = X, y\n",
    "        self._Y_proba, self._loss_op = y_proba, loss_op\n",
    "        self._training_op, self._accuracy_op = training_op, accuracy_op\n",
    "        self._init, self._saver = init, saver\n",
    "        \n",
    "    def save(self, path):\n",
    "        self._saver.save(self.session, path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "shown"
   },
   "source": [
    "Now we can use the RandomizedSearchCV class to search for the best hyperparameters. The number of iterations and the hyperparameter distributions can be tweaked depending on your computing resources:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "solution2": "shown"
   },
   "outputs": [],
   "source": [
    "from scipy.stats import randint as sp_randint\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "\n",
    "def leaky_relu(alpha=0.01):\n",
    "    def parametrized_leaky_relu(z, name=None):\n",
    "        return tf.maximum(alpha * z, z, name=name)\n",
    "    return parametrized_leaky_relu\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "mnist = MnistData(min_digit=0, max_digit=4)\n",
    "X = np.concatenate((mnist.train_images, mnist.validation_images), axis=0)\n",
    "y = np.concatenate((mnist.train_labels, mnist.validation_labels), axis=0)\n",
    "\n",
    "# specify parameters and distributions to sample from\n",
    "param_dist = {\"optimizer\": [tf.train.GradientDescentOptimizer, tf.train.AdamOptimizer,\n",
    "                            tf.train.AdagradOptimizer, partial(tf.train.MomentumOptimizer, momentum=0.9)],\n",
    "              \"hidden_layers\": sp_randint(3, 8),\n",
    "              \"num_neurons\": sp_randint(50, 250),\n",
    "              \"batch_size\": sp_randint(20, 200),\n",
    "              \"activation\": [tf.nn.elu, leaky_relu(alpha=0.01), leaky_relu(alpha=0.1)],\n",
    "              \"learning_rate\": [1e-4, 3e-4, 1e-3, 3e-3, 1e-2],\n",
    "              \"initializer\": [he_init]\n",
    "             }\n",
    "\n",
    "# run randomized search\n",
    "n_iter_search = 15\n",
    "dnn = DNNClassifier()\n",
    "random_search = RandomizedSearchCV(dnn, param_distributions=param_dist,\n",
    "                                   n_iter=n_iter_search, verbose=2)\n",
    "\n",
    "random_search.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "shown"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "y_pred = random_search.best_estimator_.predict(mnist.test_images)\n",
    "accuracy = accuracy_score(y_pred, mnist.test_labels)\n",
    "print('Accuracy of best classifier: {:.3f}'.format(accuracy * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "solution2": "shown"
   },
   "outputs": [],
   "source": [
    "random_search.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "shown"
   },
   "source": [
    "Now we can save the best model to disk, and restore it later when needed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "shown"
   },
   "outputs": [],
   "source": [
    "random_search.best_estimator_.save('./best_model_ex8.ckpt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "source": [
    "* d) Now try adding Batch Normalization and compare the learning curves: is it converging faster than before? Does it produce a better model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "dnn_with_bn = DNNClassifier(hidden_layers=5, activation=leaky_relu(alpha=0.01), initializer=he_init, \n",
    "                            num_neurons=191, optimizer=partial(tf.train.MomentumOptimizer, momentum=0.9),\n",
    "                            learning_rate=0.01, batch_size=42, batch_norm_momentum=0.9)\n",
    "dnn_with_bn.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "y_pred = dnn_with_bn.predict(mnist.test_images)\n",
    "accuracy = accuracy_score(y_pred, mnist.test_labels)\n",
    "print('Accuracy of best classifier with batch normalization: {:.3f}'.format(accuracy * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "hidden"
   },
   "source": [
    "If we use batch normalization the model will take lower to converge, slowing down training a bit. However, the performance of the model increased."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "source": [
    "* e) Is the model overfitting the training set? Try adding dropout to every layer and try again. Does it help?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "hidden"
   },
   "source": [
    "The model doesn't seem to be overfitting the training set (we even get a higher accuracy on the test set than on the validation set used for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "dnn_with_bn_and_dropout = DNNClassifier(hidden_layers=5, activation=leaky_relu(alpha=0.01), initializer=he_init, \n",
    "                            num_neurons=191, optimizer=partial(tf.train.MomentumOptimizer, momentum=0.9),\n",
    "                            learning_rate=0.01, batch_size=42, batch_norm_momentum=0.9, dropout_rate=0.5)\n",
    "dnn_with_bn_and_dropout.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "y_pred = dnn_with_bn_and_dropout.predict(mnist.test_images)\n",
    "accuracy = accuracy_score(y_pred, mnist.test_labels)\n",
    "print('Accuracy of best classifier with batch normalization and dropout: {:.3f}'.format(accuracy * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "hidden"
   },
   "source": [
    "In this case, using dropout hasn't increased the performance of our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "source": [
    "## Exercise 9\n",
    "\n",
    "Transfer learning:\n",
    "* Create a new DNN that reuses all the pretrained hidden layers of the previous model, freezes them, and replaces the softmax output layer with a new one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "saver = tf.train.import_meta_graph('best_model_ex8.ckpt.meta')\n",
    "\n",
    "graph = tf.get_default_graph()\n",
    "X = graph.get_tensor_by_name(\"x_input:0\")\n",
    "y = graph.get_tensor_by_name(\"y_input:0\")\n",
    "loss_op = graph.get_tensor_by_name(\"loss/loss:0\")\n",
    "accuracy_op = graph.get_tensor_by_name(\"accuracy/accuracy:0\")\n",
    "optimizer = tf.train.AdamOptimizer()\n",
    "train_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=\"logits\")\n",
    "training_op = optimizer.minimize(loss_op, var_list=train_vars)\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "source": [
    "* Train this new DNN on digits 5 to 9, using only 100 images per digit, and time how long it takes. Despite this small number of examples, can you achieve high precision?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "def sample_n_instances_per_class(X, y, n=100):\n",
    "    Xs, ys = [], []\n",
    "    for label in np.unique(y):\n",
    "        idx = (y == label)\n",
    "        Xc = X[idx][:n]\n",
    "        yc = y[idx][:n]\n",
    "        Xs.append(Xc)\n",
    "        ys.append(yc)\n",
    "    return np.concatenate(Xs), np.concatenate(ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "mnist_5_9 = MnistData(min_digit=5, max_digit=9)\n",
    "mnist_5_9.train_images, mnist_5_9.train_labels = sample_n_instances_per_class(mnist_5_9.train_images, \n",
    "                                                                              mnist_5_9.train_labels, 100)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, 'best_model_ex8.ckpt')\n",
    "    sess.run(init)\n",
    "\n",
    "    num_samples = 500\n",
    "    num_epochs = 1000\n",
    "    batch_size = 100\n",
    "    num_batches = num_samples // batch_size\n",
    "\n",
    "    MAX_CHECKS_NO_PROGRESS = 20\n",
    "    checks_no_progress = 0\n",
    "    best_loss = np.inf\n",
    "    for epoch in range(num_epochs):\n",
    "        for batch in range(num_batches):\n",
    "            # training step\n",
    "            X_batch, y_batch = fetch_batch(batch_size, batch, mnist_5_9.train_images, mnist_5_9.train_labels)\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "\n",
    "        loss, acc = sess.run([loss_op, accuracy_op], feed_dict={X: mnist_5_9.validation_images,\n",
    "                                                                y: mnist_5_9.validation_labels})\n",
    "        if loss < best_loss:\n",
    "            best_loss = loss\n",
    "            checks_no_progress = 0\n",
    "        else:\n",
    "            checks_no_progress += 1\n",
    "            if checks_no_progress >= MAX_CHECKS_NO_PROGRESS:\n",
    "                print(\"No progress after {} epochs. Stopping...\".format(epoch))\n",
    "                break\n",
    "        print(\"{}\\tValidation loss: {:.6f}\\tBest loss: {:.6f}\\tAccuracy: {:.3f}%\".format(\n",
    "            epoch, loss, best_loss, acc * 100))\n",
    "\n",
    "    acc_test = sess.run(accuracy_op, feed_dict={X: mnist_5_9.test_images, y: mnist_5_9.test_labels})\n",
    "    print(\"Final test accuracy: {:.3f}%\".format(acc_test * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "source": [
    "* Try caching the frozen layers, and train the model again: how much faster is it now?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "hidden5 = graph.get_tensor_by_name(\"hidden5_out:0\")\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, 'best_model_ex8.ckpt')\n",
    "    sess.run(init)\n",
    "    hidden5_cache = sess.run(hidden5, feed_dict={X: mnist_5_9.train_images, y: mnist_5_9.train_labels})\n",
    "    hidden5_val_cache = sess.run(hidden5, feed_dict={X: mnist_5_9.validation_images, y: mnist_5_9.validation_labels})\n",
    "\n",
    "    num_samples = 500\n",
    "    num_epochs = 1000\n",
    "    batch_size = 100\n",
    "    num_batches = num_samples // batch_size\n",
    "\n",
    "    MAX_CHECKS_NO_PROGRESS = 20\n",
    "    checks_no_progress = 0\n",
    "    best_loss = np.inf\n",
    "    for epoch in range(num_epochs):\n",
    "        for batch in range(num_batches):\n",
    "            # training step\n",
    "            hidden5_batch, y_batch = fetch_batch(batch_size, batch,  mnist_5_9.train_images, mnist_5_9.train_labels)\n",
    "            sess.run(training_op, feed_dict={X: hidden5_batch, y: y_batch})\n",
    "\n",
    "        loss, acc = sess.run([loss_op, accuracy_op], feed_dict={hidden5: hidden5_val_cache,\n",
    "                                                                y: mnist_5_9.validation_labels})\n",
    "        if loss < best_loss:\n",
    "            best_loss = loss\n",
    "            checks_no_progress = 0\n",
    "        else:\n",
    "            checks_no_progress += 1\n",
    "            if checks_no_progress >= MAX_CHECKS_NO_PROGRESS:\n",
    "                print(\"No progress after {} epochs. Stopping...\".format(epoch))\n",
    "                break\n",
    "        print(\"{}\\tValidation loss: {:.6f}\\tBest loss: {:.6f}\\tAccuracy: {:.3f}%\".format(\n",
    "            epoch, loss, best_loss, acc * 100))\n",
    "\n",
    "    acc_test = sess.run(accuracy_op, feed_dict={X: mnist_5_9.test_images, y: mnist_5_9.test_labels})\n",
    "    print(\"Final test accuracy: {:.3f}%\".format(acc_test * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "source": [
    "* Try again reusing just four hidden layers instead of five. Can you achieve a higher precision?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "saver = tf.train.import_meta_graph('best_model_ex8.ckpt.meta')\n",
    "n_outputs = 5\n",
    "\n",
    "graph = tf.get_default_graph()\n",
    "X = graph.get_tensor_by_name(\"x_input:0\")\n",
    "y = graph.get_tensor_by_name(\"y_input:0\")\n",
    "hidden4_out = graph.get_tensor_by_name(\"hidden4_out:0\")\n",
    "logits = tf.layers.dense(hidden4_out, n_outputs, kernel_initializer=he_init, name=\"new_logits\")\n",
    "loss_op = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits))\n",
    "softmax = tf.nn.softmax(logits)\n",
    "correct = tf.equal(tf.argmax(softmax, axis=1), y)\n",
    "accuracy_op = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer()\n",
    "train_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=\"new_logits\")\n",
    "training_op = optimizer.minimize(loss_op, var_list=train_vars)\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "mnist_5_9 = MnistData(min_digit=5, max_digit=9)\n",
    "mnist_5_9.train_images, mnist_5_9.train_labels = sample_n_instances_per_class(mnist_5_9.train_images, \n",
    "                                                                              mnist_5_9.train_labels, 100)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, 'best_model_ex8.ckpt')\n",
    "    sess.run(init)\n",
    "\n",
    "    num_samples = 500\n",
    "    num_epochs = 1000\n",
    "    batch_size = 100\n",
    "    num_batches = num_samples // batch_size\n",
    "\n",
    "    MAX_CHECKS_NO_PROGRESS = 20\n",
    "    checks_no_progress = 0\n",
    "    best_loss = np.inf\n",
    "    for epoch in range(num_epochs):\n",
    "        for batch in range(num_batches):\n",
    "            # training step\n",
    "            X_batch, y_batch = fetch_batch(batch_size, batch, mnist_5_9.train_images, mnist_5_9.train_labels)\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "\n",
    "        loss, acc = sess.run([loss_op, accuracy_op], feed_dict={X: mnist_5_9.validation_images,\n",
    "                                                                y: mnist_5_9.validation_labels})\n",
    "        if loss < best_loss:\n",
    "            best_loss = loss\n",
    "            checks_no_progress = 0\n",
    "        else:\n",
    "            checks_no_progress += 1\n",
    "            if checks_no_progress >= MAX_CHECKS_NO_PROGRESS:\n",
    "                print(\"No progress after {} epochs. Stopping...\".format(epoch))\n",
    "                break\n",
    "        print(\"{}\\tValidation loss: {:.6f}\\tBest loss: {:.6f}\\tAccuracy: {:.3f}%\".format(\n",
    "            epoch, loss, best_loss, acc * 100))\n",
    "\n",
    "    acc_test = sess.run(accuracy_op, feed_dict={X: mnist_5_9.test_images, y: mnist_5_9.test_labels})\n",
    "    print(\"Final test accuracy: {:.3f}%\".format(acc_test * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "source": [
    "* Now unfreeze the top two hidden layers and continue training: can you get the model to perform even better?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "saver = tf.train.import_meta_graph('best_model_ex8.ckpt.meta')\n",
    "n_outputs = 5\n",
    "\n",
    "graph = tf.get_default_graph()\n",
    "X = graph.get_tensor_by_name(\"x_input:0\")\n",
    "y = graph.get_tensor_by_name(\"y_input:0\")\n",
    "hidden4_out = graph.get_tensor_by_name(\"hidden4_out:0\")\n",
    "logits = tf.layers.dense(hidden4_out, n_outputs, kernel_initializer=he_init, name=\"new_logits\")\n",
    "loss_op = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits))\n",
    "softmax = tf.nn.softmax(logits)\n",
    "correct = tf.equal(tf.argmax(softmax, axis=1), y)\n",
    "accuracy_op = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer()\n",
    "train_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=\"hidden[34]|new_logits\")\n",
    "training_op = optimizer.minimize(loss_op, var_list=train_vars)\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "mnist_5_9 = MnistData(min_digit=5, max_digit=9)\n",
    "mnist_5_9.train_images, mnist_5_9.train_labels = sample_n_instances_per_class(mnist_5_9.train_images, \n",
    "                                                                              mnist_5_9.train_labels, 100)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, 'best_model_ex8.ckpt')\n",
    "    sess.run(init)\n",
    "\n",
    "    num_samples = 500\n",
    "    num_epochs = 1000\n",
    "    batch_size = 100\n",
    "    num_batches = num_samples // batch_size\n",
    "\n",
    "    MAX_CHECKS_NO_PROGRESS = 20\n",
    "    checks_no_progress = 0\n",
    "    best_loss = np.inf\n",
    "    for epoch in range(num_epochs):\n",
    "        for batch in range(num_batches):\n",
    "            # training step\n",
    "            X_batch, y_batch = fetch_batch(batch_size, batch, mnist_5_9.train_images, mnist_5_9.train_labels)\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "\n",
    "        loss, acc = sess.run([loss_op, accuracy_op], feed_dict={X: mnist_5_9.validation_images,\n",
    "                                                                y: mnist_5_9.validation_labels})\n",
    "        if loss < best_loss:\n",
    "            best_loss = loss\n",
    "            checks_no_progress = 0\n",
    "        else:\n",
    "            checks_no_progress += 1\n",
    "            if checks_no_progress >= MAX_CHECKS_NO_PROGRESS:\n",
    "                print(\"No progress after {} epochs. Stopping...\".format(epoch))\n",
    "                break\n",
    "        print(\"{}\\tValidation loss: {:.6f}\\tBest loss: {:.6f}\\tAccuracy: {:.3f}%\".format(\n",
    "            epoch, loss, best_loss, acc * 100))\n",
    "\n",
    "    acc_test = sess.run(accuracy_op, feed_dict={X: mnist_5_9.test_images, y: mnist_5_9.test_labels})\n",
    "    print(\"Final test accuracy: {:.3f}%\".format(acc_test * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 10\n",
    "\n",
    "Pretraining on an auxiliary task.\n",
    "* In this exercise you will build a DNN that compares two MNIST digit images and predicts whether they represent the same digit or not. Then you will reuse the lower layers of this network to train an MNIST classifier using very little training data. Start by building two DNNs (let's call them DNN A and B), both similar to the one you built earlier but without the output layer: each DNN should have five hidden layers of 100 neurons each, He initialization, and ELU activation. Next, add a single output layer on top of both DNNs. You should use TensorFlow's concat() function with axis=1 to concatenate the output of both DNNs along the horizontal axis, then feed the result to the output layer. This output layer should contain a single neuron using the logistic activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "def build_dnn_ex9(inputs, num_layers=5, num_neurons=100, name=\"dnn\"):\n",
    "    with tf.name_scope(name):\n",
    "        for i in range(num_layers):\n",
    "            inputs = tf.layers.dense(inputs, num_neurons, activation=tf.nn.elu,\n",
    "                                     kernel_initializer=he_init, name=\"hidden{}_{}\".format(i, name))        \n",
    "    return inputs\n",
    "\n",
    "tf.reset_default_graph()\n",
    "mnist = MnistData()\n",
    "num_inputs = np.shape(mnist.train_images)[1]\n",
    "X = tf.placeholder(tf.float32, shape=[None, 2, num_inputs], name=\"input_x\")\n",
    "X1, X2 = tf.unstack(X, axis=1)\n",
    "y = tf.placeholder(tf.float32, shape=[None, 1], name=\"input_y\")\n",
    "dnn_a = build_dnn_ex9(X1, name=\"dnn_a\")\n",
    "dnn_b = build_dnn_ex9(X2, name=\"dnn_b\")\n",
    "dnn_outputs = tf.concat([dnn_a, dnn_b], axis=1)\n",
    "hidden = tf.layers.dense(dnn_outputs, units=10, activation=tf.nn.elu, kernel_initializer=he_init)\n",
    "output = tf.layers.dense(hidden, units=1, activation=None, kernel_initializer=he_init, name=\"logits\")\n",
    "y_proba = tf.nn.sigmoid(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Split the MNIST training set in two sets: split #1 should contain 55,000 images, and split #2 should contain 5,000 images. Create a function that generates a training batch where each instance is a pair of MNIST images picked from split #1. Half of the training instances should be pairs of images that belong to the same class, while the other half should be images from different classes. For each pair, the training label should be 0 if the images are from the same class, or 1 if they are from different classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MnistData class already creates a training and validation set, with sizes 55,0000 and 5,000 respectively. So that part is already done. We will create the function that generates the training batch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_training_batch_ex9(X, y, batch_size):\n",
    "    newX, newY = [], []\n",
    "    \n",
    "    while len(newX) != batch_size // 2:\n",
    "        rnd_idx1, rnd_idx2 = np.random.randint(0, len(X), 2)\n",
    "        if rnd_idx1 == rnd_idx2:\n",
    "            continue\n",
    "            \n",
    "        if y[rnd_idx1] == y[rnd_idx2]:\n",
    "            newX.append(np.array([X[rnd_idx1], X[rnd_idx2]]))\n",
    "            newY.append([0])\n",
    "            \n",
    "    while len(newX) != batch_size:\n",
    "        rnd_idx1, rnd_idx2 = np.random.randint(0, len(X), 2)\n",
    "        if rnd_idx1 == rnd_idx2:\n",
    "            continue\n",
    "            \n",
    "        if y[rnd_idx1] != y[rnd_idx2]:\n",
    "            newX.append(np.array([X[rnd_idx1], X[rnd_idx2]]))\n",
    "            newY.append([1])\n",
    "    rnd_indices = np.random.permutation(batch_size)\n",
    "    return np.array(newX)[rnd_indices], np.array(newY)[rnd_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Train the DNN on this training set. For each image pair, you can simultaneously feed the first image to DNN A and the second image to DNN B. The whole network will gradually learn to tell whether two images belond to the same class or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Test loss: 0.767728 Test accuracy: 49.800%\n",
      "5 Test loss: 0.696826 Test accuracy: 52.200%\n",
      "10 Test loss: 0.672249 Test accuracy: 56.500%\n",
      "15 Test loss: 0.619001 Test accuracy: 63.570%\n",
      "20 Test loss: 0.583510 Test accuracy: 67.110%\n",
      "25 Test loss: 0.548223 Test accuracy: 69.660%\n",
      "30 Test loss: 0.527159 Test accuracy: 71.710%\n",
      "35 Test loss: 0.473895 Test accuracy: 75.930%\n",
      "40 Test loss: 0.436548 Test accuracy: 79.490%\n",
      "45 Test loss: 0.390578 Test accuracy: 82.740%\n",
      "50 Test loss: 0.372942 Test accuracy: 83.940%\n",
      "55 Test loss: 0.358710 Test accuracy: 83.950%\n",
      "60 Test loss: 0.347274 Test accuracy: 84.870%\n",
      "65 Test loss: 0.324885 Test accuracy: 85.790%\n",
      "70 Test loss: 0.361298 Test accuracy: 84.050%\n",
      "75 Test loss: 0.333711 Test accuracy: 85.710%\n",
      "80 Test loss: 0.311378 Test accuracy: 86.410%\n",
      "85 Test loss: 0.329751 Test accuracy: 85.960%\n",
      "90 Test loss: 0.297573 Test accuracy: 87.260%\n",
      "95 Test loss: 0.302573 Test accuracy: 87.200%\n",
      "100 Test loss: 0.301963 Test accuracy: 87.270%\n",
      "105 Test loss: 0.294284 Test accuracy: 87.480%\n",
      "110 Test loss: 0.308775 Test accuracy: 86.930%\n",
      "115 Test loss: 0.292237 Test accuracy: 88.160%\n",
      "120 Test loss: 0.289702 Test accuracy: 88.020%\n",
      "125 Test loss: 0.269200 Test accuracy: 89.140%\n",
      "130 Test loss: 0.283159 Test accuracy: 88.030%\n",
      "135 Test loss: 0.270679 Test accuracy: 88.710%\n",
      "140 Test loss: 0.254087 Test accuracy: 89.790%\n",
      "145 Test loss: 0.270712 Test accuracy: 89.030%\n",
      "150 Test loss: 0.252425 Test accuracy: 89.960%\n",
      "155 Test loss: 0.266439 Test accuracy: 89.120%\n",
      "160 Test loss: 0.248968 Test accuracy: 89.960%\n",
      "165 Test loss: 0.254216 Test accuracy: 90.130%\n",
      "170 Test loss: 0.250068 Test accuracy: 90.000%\n",
      "175 Test loss: 0.250447 Test accuracy: 90.280%\n",
      "180 Test loss: 0.235513 Test accuracy: 90.830%\n",
      "185 Test loss: 0.246747 Test accuracy: 90.590%\n",
      "190 Test loss: 0.228677 Test accuracy: 91.350%\n",
      "195 Test loss: 0.226652 Test accuracy: 91.340%\n",
      "200 Test loss: 0.223774 Test accuracy: 91.590%\n",
      "205 Test loss: 0.214747 Test accuracy: 91.810%\n",
      "210 Test loss: 0.225994 Test accuracy: 91.490%\n",
      "215 Test loss: 0.215422 Test accuracy: 91.940%\n",
      "220 Test loss: 0.225702 Test accuracy: 91.750%\n",
      "225 Test loss: 0.218751 Test accuracy: 91.270%\n",
      "230 Test loss: 0.208390 Test accuracy: 92.220%\n",
      "235 Test loss: 0.215975 Test accuracy: 92.020%\n",
      "240 Test loss: 0.217946 Test accuracy: 91.730%\n",
      "245 Test loss: 0.209325 Test accuracy: 91.890%\n",
      "250 Test loss: 0.203530 Test accuracy: 92.370%\n",
      "255 Test loss: 0.192842 Test accuracy: 92.700%\n",
      "260 Test loss: 0.204483 Test accuracy: 92.430%\n",
      "265 Test loss: 0.198587 Test accuracy: 92.710%\n",
      "270 Test loss: 0.214956 Test accuracy: 91.920%\n",
      "275 Test loss: 0.200534 Test accuracy: 92.570%\n",
      "280 Test loss: 0.194048 Test accuracy: 92.820%\n",
      "285 Test loss: 0.185068 Test accuracy: 93.290%\n",
      "290 Test loss: 0.199730 Test accuracy: 92.700%\n",
      "295 Test loss: 0.182048 Test accuracy: 93.340%\n",
      "300 Test loss: 0.188179 Test accuracy: 92.780%\n",
      "305 Test loss: 0.189243 Test accuracy: 93.240%\n",
      "310 Test loss: 0.188251 Test accuracy: 93.050%\n",
      "315 Test loss: 0.191307 Test accuracy: 92.660%\n",
      "320 Test loss: 0.195005 Test accuracy: 92.900%\n",
      "325 Test loss: 0.167113 Test accuracy: 94.120%\n",
      "330 Test loss: 0.185614 Test accuracy: 93.270%\n",
      "335 Test loss: 0.180709 Test accuracy: 93.520%\n",
      "340 Test loss: 0.179261 Test accuracy: 93.710%\n",
      "345 Test loss: 0.166111 Test accuracy: 93.710%\n",
      "350 Test loss: 0.173970 Test accuracy: 93.730%\n",
      "355 Test loss: 0.171882 Test accuracy: 94.020%\n",
      "360 Test loss: 0.168355 Test accuracy: 93.930%\n",
      "365 Test loss: 0.171365 Test accuracy: 93.650%\n",
      "370 Test loss: 0.165374 Test accuracy: 94.230%\n",
      "375 Test loss: 0.172392 Test accuracy: 93.950%\n",
      "380 Test loss: 0.162814 Test accuracy: 94.420%\n",
      "385 Test loss: 0.160443 Test accuracy: 94.430%\n",
      "390 Test loss: 0.162476 Test accuracy: 94.300%\n",
      "395 Test loss: 0.162497 Test accuracy: 94.090%\n",
      "400 Test loss: 0.166513 Test accuracy: 94.040%\n",
      "405 Test loss: 0.176935 Test accuracy: 93.640%\n",
      "410 Test loss: 0.180534 Test accuracy: 93.340%\n",
      "415 Test loss: 0.160084 Test accuracy: 94.210%\n",
      "420 Test loss: 0.157813 Test accuracy: 94.520%\n",
      "425 Test loss: 0.155691 Test accuracy: 94.430%\n",
      "430 Test loss: 0.154678 Test accuracy: 94.590%\n",
      "435 Test loss: 0.145791 Test accuracy: 94.960%\n",
      "440 Test loss: 0.141267 Test accuracy: 95.040%\n",
      "445 Test loss: 0.141937 Test accuracy: 95.370%\n",
      "450 Test loss: 0.135990 Test accuracy: 95.320%\n",
      "455 Test loss: 0.140335 Test accuracy: 95.270%\n",
      "460 Test loss: 0.137009 Test accuracy: 95.460%\n",
      "465 Test loss: 0.135955 Test accuracy: 95.150%\n",
      "470 Test loss: 0.136964 Test accuracy: 95.460%\n",
      "475 Test loss: 0.152759 Test accuracy: 94.700%\n",
      "480 Test loss: 0.141701 Test accuracy: 94.920%\n",
      "485 Test loss: 0.133830 Test accuracy: 95.240%\n",
      "490 Test loss: 0.137331 Test accuracy: 95.410%\n",
      "495 Test loss: 0.135524 Test accuracy: 95.310%\n",
      "500 Test loss: 0.142088 Test accuracy: 95.140%\n",
      "505 Test loss: 0.139172 Test accuracy: 95.210%\n",
      "510 Test loss: 0.134074 Test accuracy: 95.660%\n",
      "515 Test loss: 0.141452 Test accuracy: 95.290%\n",
      "520 Test loss: 0.132454 Test accuracy: 95.570%\n",
      "525 Test loss: 0.133182 Test accuracy: 95.630%\n",
      "530 Test loss: 0.144452 Test accuracy: 95.140%\n",
      "535 Test loss: 0.136884 Test accuracy: 95.450%\n",
      "540 Test loss: 0.137498 Test accuracy: 95.310%\n",
      "545 Test loss: 0.125998 Test accuracy: 95.950%\n",
      "550 Test loss: 0.127724 Test accuracy: 95.910%\n",
      "555 Test loss: 0.129703 Test accuracy: 95.770%\n",
      "560 Test loss: 0.124263 Test accuracy: 95.880%\n",
      "565 Test loss: 0.130342 Test accuracy: 95.640%\n",
      "570 Test loss: 0.122092 Test accuracy: 95.900%\n",
      "575 Test loss: 0.124088 Test accuracy: 95.840%\n",
      "580 Test loss: 0.111724 Test accuracy: 96.400%\n",
      "585 Test loss: 0.122189 Test accuracy: 95.950%\n",
      "590 Test loss: 0.123313 Test accuracy: 96.100%\n",
      "595 Test loss: 0.124618 Test accuracy: 95.880%\n",
      "600 Test loss: 0.110766 Test accuracy: 96.490%\n",
      "605 Test loss: 0.125879 Test accuracy: 95.800%\n",
      "610 Test loss: 0.113783 Test accuracy: 96.030%\n",
      "615 Test loss: 0.128465 Test accuracy: 95.620%\n",
      "620 Test loss: 0.121107 Test accuracy: 95.770%\n",
      "625 Test loss: 0.111875 Test accuracy: 96.250%\n",
      "630 Test loss: 0.115571 Test accuracy: 96.170%\n",
      "635 Test loss: 0.115808 Test accuracy: 96.160%\n",
      "640 Test loss: 0.115484 Test accuracy: 96.230%\n",
      "645 Test loss: 0.108905 Test accuracy: 96.260%\n",
      "650 Test loss: 0.121106 Test accuracy: 95.940%\n",
      "655 Test loss: 0.107094 Test accuracy: 96.510%\n",
      "660 Test loss: 0.116531 Test accuracy: 96.110%\n",
      "665 Test loss: 0.121876 Test accuracy: 95.630%\n",
      "670 Test loss: 0.125151 Test accuracy: 95.870%\n",
      "675 Test loss: 0.114651 Test accuracy: 96.300%\n",
      "680 Test loss: 0.112890 Test accuracy: 96.470%\n",
      "685 Test loss: 0.105445 Test accuracy: 96.630%\n",
      "690 Test loss: 0.110940 Test accuracy: 96.260%\n",
      "695 Test loss: 0.120539 Test accuracy: 95.970%\n",
      "700 Test loss: 0.123775 Test accuracy: 95.860%\n",
      "705 Test loss: 0.116746 Test accuracy: 96.180%\n",
      "710 Test loss: 0.120444 Test accuracy: 95.990%\n",
      "715 Test loss: 0.104533 Test accuracy: 96.490%\n",
      "720 Test loss: 0.114943 Test accuracy: 96.200%\n",
      "725 Test loss: 0.129246 Test accuracy: 95.660%\n",
      "730 Test loss: 0.105249 Test accuracy: 96.650%\n",
      "735 Test loss: 0.108248 Test accuracy: 96.540%\n",
      "740 Test loss: 0.121980 Test accuracy: 96.030%\n",
      "745 Test loss: 0.120903 Test accuracy: 95.890%\n",
      "750 Test loss: 0.114125 Test accuracy: 96.390%\n",
      "755 Test loss: 0.108709 Test accuracy: 96.390%\n",
      "760 Test loss: 0.105454 Test accuracy: 96.490%\n",
      "765 Test loss: 0.106765 Test accuracy: 96.420%\n",
      "770 Test loss: 0.105313 Test accuracy: 96.430%\n",
      "775 Test loss: 0.120206 Test accuracy: 96.000%\n",
      "780 Test loss: 0.101716 Test accuracy: 96.660%\n",
      "785 Test loss: 0.113976 Test accuracy: 95.860%\n",
      "790 Test loss: 0.106261 Test accuracy: 96.540%\n",
      "795 Test loss: 0.102804 Test accuracy: 96.660%\n",
      "800 Test loss: 0.108856 Test accuracy: 96.440%\n",
      "805 Test loss: 0.106583 Test accuracy: 96.660%\n",
      "810 Test loss: 0.109211 Test accuracy: 96.320%\n",
      "815 Test loss: 0.098453 Test accuracy: 96.900%\n",
      "820 Test loss: 0.105712 Test accuracy: 96.600%\n",
      "825 Test loss: 0.098717 Test accuracy: 96.890%\n",
      "830 Test loss: 0.105187 Test accuracy: 96.640%\n",
      "835 Test loss: 0.100988 Test accuracy: 96.750%\n",
      "840 Test loss: 0.099850 Test accuracy: 96.930%\n",
      "845 Test loss: 0.108290 Test accuracy: 96.570%\n",
      "850 Test loss: 0.108699 Test accuracy: 96.840%\n",
      "855 Test loss: 0.111662 Test accuracy: 96.410%\n",
      "860 Test loss: 0.101123 Test accuracy: 96.640%\n",
      "865 Test loss: 0.097319 Test accuracy: 96.980%\n",
      "870 Test loss: 0.098039 Test accuracy: 96.910%\n",
      "875 Test loss: 0.101600 Test accuracy: 96.630%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "880 Test loss: 0.098396 Test accuracy: 96.850%\n",
      "885 Test loss: 0.104390 Test accuracy: 96.570%\n",
      "890 Test loss: 0.104620 Test accuracy: 96.540%\n",
      "895 Test loss: 0.101509 Test accuracy: 96.780%\n",
      "900 Test loss: 0.107651 Test accuracy: 96.390%\n",
      "905 Test loss: 0.102775 Test accuracy: 96.610%\n",
      "910 Test loss: 0.103319 Test accuracy: 96.580%\n",
      "915 Test loss: 0.094663 Test accuracy: 96.970%\n",
      "920 Test loss: 0.094775 Test accuracy: 97.080%\n",
      "925 Test loss: 0.095587 Test accuracy: 96.880%\n",
      "930 Test loss: 0.090691 Test accuracy: 96.970%\n",
      "935 Test loss: 0.091197 Test accuracy: 97.000%\n",
      "940 Test loss: 0.092320 Test accuracy: 97.190%\n",
      "945 Test loss: 0.087818 Test accuracy: 97.240%\n",
      "950 Test loss: 0.091942 Test accuracy: 97.010%\n",
      "955 Test loss: 0.086720 Test accuracy: 97.370%\n",
      "960 Test loss: 0.094883 Test accuracy: 96.760%\n",
      "965 Test loss: 0.090593 Test accuracy: 97.070%\n",
      "970 Test loss: 0.084623 Test accuracy: 97.330%\n",
      "975 Test loss: 0.089189 Test accuracy: 97.270%\n",
      "980 Test loss: 0.091825 Test accuracy: 97.020%\n",
      "985 Test loss: 0.099930 Test accuracy: 96.820%\n",
      "990 Test loss: 0.092030 Test accuracy: 96.950%\n",
      "995 Test loss: 0.087279 Test accuracy: 97.110%\n",
      "Final test accuracy: 96.980%\n"
     ]
    }
   ],
   "source": [
    "X_test, y_test = generate_training_batch_ex9(mnist.test_images, mnist.test_labels, len(mnist.test_images))\n",
    "\n",
    "with tf.name_scope('loss'):\n",
    "    loss_op = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=y, logits=output))\n",
    "\n",
    "with tf.name_scope('train'):\n",
    "    optimizer = tf.train.AdamOptimizer()\n",
    "    training_op = optimizer.minimize(loss_op)\n",
    "\n",
    "with tf.name_scope('accuracy'):\n",
    "    y_pred = tf.cast(tf.greater_equal(y_proba, 0.5), tf.float32)\n",
    "    correct = tf.equal(y_pred, y)\n",
    "    accuracy_op = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    num_samples = 500\n",
    "    num_epochs = 1000\n",
    "    batch_size = 100\n",
    "    num_batches = num_samples // batch_size\n",
    "\n",
    "    MAX_CHECKS_NO_PROGRESS = 20\n",
    "    checks_no_progress = 0\n",
    "    best_loss = np.inf\n",
    "    for epoch in range(num_epochs):\n",
    "        for batch in range(num_batches):\n",
    "            # training step\n",
    "            X_batch, y_batch = generate_training_batch_ex9(mnist.train_images, mnist.train_labels, batch_size)\n",
    "            _, loss_val = sess.run([training_op, loss_op], feed_dict={X: X_batch, y: y_batch})\n",
    "        if epoch % 5 == 0:\n",
    "            loss, acc = sess.run([loss_op, accuracy_op], feed_dict={X: X_test,\n",
    "                                                                    y: y_test})\n",
    "            print(\"{} Test loss: {:.6f} Test accuracy: {:.3f}%\".format(\n",
    "                  epoch, loss, acc * 100))\n",
    "\n",
    "    acc_test = sess.run(accuracy_op, feed_dict={X: X_test, y: y_test})\n",
    "    print(\"Final test accuracy: {:.3f}%\".format(acc_test * 100))\n",
    "    \n",
    "    save_path = saver.save(sess, \"./my_digit_comparison_model.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Now create a new DNN by reusing and freezing the hidden layers of DNN A, and adding a softmax output layer on with 10 neurons. Train this network on split #2 and see if you can achieve high performance despite having only 500 images per class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "n_inputs = 28 * 28  # MNIST\n",
    "n_outputs = 10\n",
    "\n",
    "restore_saver = tf.train.import_meta_graph('my_digit_comparison_model.ckpt.meta')\n",
    "\n",
    "graph = tf.get_default_graph()\n",
    "X = graph.get_tensor_by_name(\"input_x:0\")\n",
    "y = tf.placeholder(tf.int64, [None])\n",
    "last_hidden = graph.get_tensor_by_name(\"dnn_a/hidden4_dnn_a/Elu:0\")\n",
    "new_output = tf.layers.dense(last_hidden, units=10, activation=None, kernel_initializer=he_init, name=\"new_logits\")\n",
    "\n",
    "with tf.name_scope('loss'):\n",
    "    loss_op = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=new_output))\n",
    "\n",
    "with tf.name_scope('train'):\n",
    "    optimizer = tf.train.AdamOptimizer()\n",
    "    train_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=\"new_logits\")\n",
    "    training_op = optimizer.minimize(loss_op, var_list=train_vars)\n",
    "\n",
    "with tf.name_scope('accuracy'):\n",
    "    y_proba = tf.nn.softmax(new_output, name=\"y_proba\")\n",
    "    correct = tf.equal(tf.argmax(y_proba, axis=1), y)\n",
    "    accuracy_op = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "INFO:tensorflow:Restoring parameters from ./my_digit_comparison_model.ckpt\n",
      "0 Test accuracy: 0.9242\n",
      "10 Test accuracy: 0.9667\n",
      "20 Test accuracy: 0.9661\n",
      "30 Test accuracy: 0.9672\n",
      "40 Test accuracy: 0.967\n",
      "50 Test accuracy: 0.9675\n",
      "60 Test accuracy: 0.9677\n",
      "70 Test accuracy: 0.968\n",
      "80 Test accuracy: 0.9652\n",
      "90 Test accuracy: 0.9668\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 100\n",
    "batch_size = 50\n",
    "\n",
    "mnist = MnistData()\n",
    "X_test, y_test = mnist.test_images, mnist.test_labels\n",
    "X_train2, y_train2 = mnist.validation_images, mnist.validation_labels\n",
    "X_train2 = np.stack((X_train2, np.zeros([5000, 784])), axis=1)\n",
    "X_test = np.stack((X_test, np.zeros([10000, 784])), axis=1)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    restore_saver.restore(sess, \"./my_digit_comparison_model.ckpt\")\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        rnd_idx = np.random.permutation(len(X_train2))\n",
    "        for rnd_indices in np.array_split(rnd_idx, len(X_train2) // batch_size):\n",
    "            X_batch, y_batch = X_train2[rnd_indices], y_train2[rnd_indices]\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        if epoch % 10 == 0:\n",
    "            acc_test = accuracy_op.eval(feed_dict={X: X_test, y: y_test})\n",
    "            print(epoch, \"Test accuracy:\", acc_test)\n",
    "\n",
    "    save_path = saver.save(sess, \"./my_mnist_model_final.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0],\n",
       "       [1],\n",
       "       [1],\n",
       "       ...,\n",
       "       [0],\n",
       "       [0],\n",
       "       [0]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
