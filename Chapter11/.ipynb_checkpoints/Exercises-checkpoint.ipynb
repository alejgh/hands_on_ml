{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 11: Training Deep Neural Nets "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "source": [
    "## Exercise 1\n",
    "Is it okay to initialize all the weights to the same value as long as that value is selected randomly using He initialization?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "hidden"
   },
   "source": [
    "No. If you initiaze all the weights with the same value, even if it is obtained using He initialization, you won't break the simmetry of each layer. The neural network will behave as if it had just one neuron per layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "source": [
    "## Exercise 2\n",
    "Is it okay to initialize the bias terms to 0?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "hidden"
   },
   "source": [
    "Yes, it is ok."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "source": [
    "## Exercise 3\n",
    "Name three advantages of the ELU activation function over ReLU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "hidden"
   },
   "source": [
    "* It has a non-zero gradient when z < 0, which avoid the dying units problem.\n",
    "* The function is smooth everywhere, which speeds up gradient descent, since it does not bounce so much left and right of z = 0\n",
    "* It takes negative values when z < 0, which allows the unit to have an average output closer to 0. This helps alleviate the vanishing gradients problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "source": [
    "## Exercise 4\n",
    "In which cases would you want to use each of the following activation functions: ELU, leaky ReLU (and its variants), ReLU, tanh, logistic, and softmax?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "hidden"
   },
   "source": [
    "* ELU: Almost always. Only drawback is that the ELU function is quite slow to compute.\n",
    "* Leaky ReLU: To avoid the dying units problem that ReLU has.\n",
    "* ReLU: Need speed. Good default, but ELU and Leaky ReLU can be better.\n",
    "* Tanh: If you need to output a number between 1 and -1. Rarely used.\n",
    "* Logistic: To estimate probabilities. Also rarely used.\n",
    "* Softmax: You need to output probabilities of mutually exclusive classes. Usually used in the output layer for classification tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "source": [
    "## Exercise 5\n",
    "What may happen if you set the momentum hyperparameter too close to 1 (e.g. 0.99999) when using a MomentumOptimizer?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "hidden"
   },
   "source": [
    "If you set the momentum hyperparameter too close to 1 the system will have almost no friction, so the gradient steps can get too high and the system may not converge to a good solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "source": [
    "## Exercise 6\n",
    "Name three ways you can produce a sparse model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "hidden"
   },
   "source": [
    "* Setting to 0 all the weights with really small values.\n",
    "* Using a high $l1$ regularization during training, which will force the optimizer to zero out as many weights as it can.\n",
    "* Applying other techniques, such as Follow The Regularized Leader."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "source": [
    "## Exercise 7\n",
    "Does dropout slow down training? Does it slow down inference (i.e. making predictions on new instances)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "hidden"
   },
   "source": [
    "Dropout will slow training a bit, but inference will be the same (you only have to multiply the output of each neuron by the keep ratio)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "source": [
    "## Exercise 8\n",
    "Deep Learning\n",
    "* a) Build a DNN with five hidden layers of 100 neurons each, He initialization, and the ELU activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "he_init = tf.contrib.layers.variance_scaling_initializer()\n",
    "\n",
    "def build_dnn_ex8a(X):\n",
    "    hidden_1 = tf.layers.dense(X, 100, activation=tf.nn.elu, kernel_initializer=he_init, name=\"hidden1\")\n",
    "    hidden_2 = tf.layers.dense(hidden_1, 100, activation=tf.nn.elu, kernel_initializer=he_init, name=\"hidden2\")\n",
    "    hidden_3 = tf.layers.dense(hidden_2, 100, activation=tf.nn.elu, kernel_initializer=he_init, name=\"hidden3\")\n",
    "    hidden_4 = tf.layers.dense(hidden_3, 100, activation=tf.nn.elu, kernel_initializer=he_init, name=\"hidden4\")\n",
    "    hidden_5 = tf.layers.dense(hidden_4, 100, activation=tf.nn.elu, kernel_initializer=he_init, name=\"hidden5\")\n",
    "    return hidden_5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "source": [
    "* b) Using Adam optimization and early stopping, try training it on MNIST but only on digits 0 to 4, as we will use transfer learning for digits 5 to 9 in the next exercise. You will need a softmax output layer with five neurons, and as always make sure to save checkpoints at regular intervals and save the final model so you can reuse it later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "\n",
    "class MnistData():\n",
    "    def __init__(self, min_digit=0, max_digit=9):\n",
    "        mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "        digit_filter = np.vectorize(lambda t: t >= min_digit and t <= max_digit)\n",
    "        train_idx = digit_filter(np.argmax(mnist.train.labels, axis=1))\n",
    "        validation_idx = digit_filter(np.argmax(mnist.validation.labels, axis=1))\n",
    "        test_idx = digit_filter(np.argmax(mnist.test.labels, axis=1))\n",
    "        self.train_images = mnist.train.images[train_idx]\n",
    "        self.train_labels = np.argmax(mnist.train.labels[train_idx, min_digit:max_digit+1], axis=1)\n",
    "        self.validation_images = mnist.validation.images[validation_idx]\n",
    "        self.validation_labels = np.argmax(mnist.validation.labels[validation_idx, min_digit:max_digit+1], axis=1)\n",
    "        self.test_images = mnist.test.images[test_idx]\n",
    "        self.test_labels = np.argmax(mnist.test.labels[test_idx, min_digit:max_digit+1], axis=1)\n",
    "        \n",
    "def fetch_batch(batch_size, batch_idx, X, y):\n",
    "    start = batch_idx * batch_size\n",
    "    end = (batch_idx + 1) * batch_size\n",
    "    batch_x = X[start:end]\n",
    "    batch_y = y[start:end]\n",
    "    return batch_x, batch_y\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# build dnn\n",
    "mnist = MnistData(min_digit=0, max_digit=4)\n",
    "num_samples = np.shape(mnist.train_images)[0]\n",
    "num_classes = np.shape(np.unique(mnist.train_labels))[0]\n",
    "num_features = np.shape(mnist.train_images)[1]\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=[None, num_features], name=\"x_input\")\n",
    "y = tf.placeholder(tf.int64, shape=[None], name=\"y_input\")\n",
    "dnn = build_dnn_ex8a(X)\n",
    "output = tf.layers.dense(dnn, num_classes, activation=None, kernel_initializer=he_init, name=\"logits\")\n",
    "\n",
    "# training  \n",
    "with tf.name_scope('loss'):\n",
    "    loss_op = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=output))\n",
    "\n",
    "with tf.name_scope('train'):\n",
    "    optimizer = tf.train.AdamOptimizer()\n",
    "    training_op = optimizer.minimize(loss_op)\n",
    "\n",
    "with tf.name_scope('accuracy'):\n",
    "    softmax = tf.nn.softmax(output)\n",
    "    correct = tf.equal(tf.argmax(softmax, axis=1), y)\n",
    "    accuracy_op = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "    \n",
    "saver = tf.train.Saver()\n",
    "initializer = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(initializer)\n",
    "\n",
    "    num_epochs = 1000\n",
    "    batch_size = 100\n",
    "    num_batches = num_samples // batch_size\n",
    "\n",
    "    MAX_CHECKS_NO_PROGRESS = 20\n",
    "    checks_no_progress = 0\n",
    "    best_loss = np.inf\n",
    "    for epoch in range(num_epochs):\n",
    "        for batch in range(num_batches):\n",
    "            # training step\n",
    "            X_batch, y_batch = fetch_batch(batch_size, batch, mnist.train_images, mnist.train_labels)\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "\n",
    "        loss, acc = sess.run([loss_op, accuracy_op], feed_dict={X: mnist.validation_images, y: mnist.validation_labels})\n",
    "        if loss < best_loss:\n",
    "            best_loss = loss\n",
    "            checks_no_progress = 0\n",
    "            saver.save(sess, \"./mnist_digits_0-4.ckpt\")\n",
    "        else:\n",
    "            checks_no_progress += 1\n",
    "            if checks_no_progress >= MAX_CHECKS_NO_PROGRESS:\n",
    "                print(\"No progress after {} epochs. Stopping...\".format(epoch))\n",
    "                break\n",
    "        print(\"{}\\tValidation loss: {:.6f}\\tBest loss: {:.6f}\\tAccuracy: {:.3f}%\".format(\n",
    "            epoch, loss, best_loss, acc * 100))\n",
    "\n",
    "    saver.restore(sess, \"./mnist_digits_0-4.ckpt\")\n",
    "    acc_test = accuracy_op.eval(feed_dict={X: mnist.test_images, y: mnist.test_labels})\n",
    "    print(\"Final test accuracy: {:.3f}%\".format(acc_test * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "source": [
    "* c) Tune the hyperparameters using cross-validation and see what precision you can achieve."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "hidden"
   },
   "source": [
    "First of all we will move most of the code from before to a custom class which will hold all the hyperparameters that can be tweaked. After doing that, we will be able to use the RandomizedCV class from scikitlearn in order to obtain easily the best hyperparameters for our DNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.exceptions import NotFittedError\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class DNNClassifier(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, hidden_layers=5, num_neurons=100, optimizer=tf.train.AdamOptimizer,\n",
    "                 batch_size=50, learning_rate=1e-4, activation=tf.nn.elu, initializer=he_init,\n",
    "                 batch_norm_momentum=None, dropout_rate=None, tensorboard_logdir=None, random_seed=42):\n",
    "        self.hidden_layers = hidden_layers\n",
    "        self.num_neurons = num_neurons\n",
    "        self.optimizer = optimizer\n",
    "        self.batch_size = batch_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.activation = activation\n",
    "        self.initializer = initializer\n",
    "        self.batch_norm_momentum = batch_norm_momentum\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.tensorboard_logdir = None\n",
    "        self.random_seed = random_seed\n",
    "        self.session = None\n",
    "    \n",
    "    def fit(self, X, y, num_epochs=1000):\n",
    "        X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.25, random_state=self.random_seed)\n",
    "        \n",
    "        num_features = np.shape(X)[1]\n",
    "        classes = np.unique(y)\n",
    "        num_classes = np.shape(classes)[0]\n",
    "        \n",
    "        self._graph = tf.Graph()\n",
    "        with self._graph.as_default():\n",
    "            self._build_graph(num_features, num_classes)\n",
    "        self.session = tf.Session(graph=self._graph)\n",
    "        self.session.run(self._init)\n",
    "\n",
    "        num_samples = np.shape(X_train)[0]\n",
    "        num_batches = num_samples // self.batch_size\n",
    "        MAX_CHECKS_NO_PROGRESS = 20\n",
    "        checks_no_progress = 0\n",
    "        best_loss = np.inf\n",
    "        with self.session.as_default() as sess:\n",
    "            for epoch in range(num_epochs):\n",
    "                for batch in range(num_batches):\n",
    "                    # training step\n",
    "                    X_batch, y_batch = fetch_batch(batch_size, batch, X_train, y_train)\n",
    "                    sess.run(self._training_op, feed_dict={self._X: X_batch, self._y: y_batch})\n",
    "                    if self.tensorboard_logdir is not None and batch_idx % 3 == 0:\n",
    "                        step = epoch * num_batches + batch\n",
    "                        s = self.session.run(self.summaries, feed_dict={self._X: X_batch,\n",
    "                                                                        self._y: y_batch})\n",
    "                        self.writer.add_summary(s, step)\n",
    "\n",
    "                loss, acc = sess.run([self._loss_op, self._accuracy_op], feed_dict={self._X: X_val, self._y: y_val})\n",
    "                if loss < best_loss:\n",
    "                    best_loss = loss\n",
    "                    checks_no_progress = 0\n",
    "                else:\n",
    "                    checks_no_progress += 1\n",
    "                    if checks_no_progress >= MAX_CHECKS_NO_PROGRESS:\n",
    "                        print(\"No progress after {} epochs. Stopping...\".format(epoch))\n",
    "                        break\n",
    "                print(\"{}\\tValidation loss: {:.6f}\\tBest loss: {:.6f}\\tAccuracy: {:.3f}%\".format(\n",
    "                    epoch, loss, best_loss, acc * 100))\n",
    "    \n",
    "    def predict(self, X):\n",
    "        if self.session is None:\n",
    "            raise NotFittedError()\n",
    "        else:\n",
    "            probabilities = self._predict_proba(X)\n",
    "            return np.argmax(probabilities, axis=1)\n",
    "\n",
    "    \n",
    "    def _predict_proba(self, X):\n",
    "        with self.session.as_default() as sess:\n",
    "            return self._Y_proba.eval(feed_dict={self._X: X})\n",
    "    \n",
    "    def _dnn(self, inputs):\n",
    "        for layer in range(self.hidden_layers):\n",
    "            if self.dropout_rate:\n",
    "                inputs = tf.layers.dropout(inputs, self.dropout_rate, training=self._training, seed=self.random_seed)\n",
    "            \n",
    "            inputs = tf.layers.dense(inputs, self.num_neurons, \n",
    "                                     kernel_initializer=self.initializer, name=\"hidden%d\" % (layer + 1))\n",
    "            if self.batch_norm_momentum:\n",
    "                inputs = tf.layers.batch_normalization(inputs, momentum=self.batch_norm_momentum)\n",
    "                \n",
    "            inputs = self.activation(inputs, name=\"hidden%d_out\" % (layer + 1))\n",
    "            \n",
    "        return inputs\n",
    "    \n",
    "    def _build_graph(self, num_inputs, num_outputs):\n",
    "        if self.random_seed is not None:\n",
    "            tf.set_random_seed(self.random_seed)\n",
    "            np.random.seed(self.random_seed)\n",
    "\n",
    "        X = tf.placeholder(tf.float32, shape=[None, num_inputs], name=\"x_input\")\n",
    "        y = tf.placeholder(tf.int64, shape=[None], name=\"y_input\")\n",
    "        if self.batch_norm_momentum or self.dropout_rate:\n",
    "            self._training = tf.placeholder_with_default(False, shape=(), name='training')\n",
    "        else:\n",
    "            self._training = None\n",
    "        \n",
    "        dnn = self._dnn(X)\n",
    "        logits = tf.layers.dense(dnn, num_outputs, activation=None, \n",
    "                                      kernel_initializer=self.initializer, name=\"logits\")\n",
    "        with tf.name_scope('loss'):\n",
    "            loss_op = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits), name=\"loss\")\n",
    "\n",
    "        with tf.name_scope('train'):\n",
    "            optimizer = self.optimizer(self.learning_rate)\n",
    "            training_op = optimizer.minimize(loss_op, name=\"training\")\n",
    "\n",
    "        with tf.name_scope('accuracy'):\n",
    "            y_proba = tf.nn.softmax(logits, name=\"y_proba\")\n",
    "            correct = tf.equal(tf.argmax(y_proba, axis=1), y)\n",
    "            accuracy_op = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")\n",
    "            \n",
    "        if self.tensorboard_logdir is not None:\n",
    "            now = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
    "            log_dir = os.path.join(self.tensorboard_logdir, \"run-{}\".format(now))\n",
    "            self.writer = tf.summary.FileWriter(log_dir)\n",
    "            tf.summary.scalar('loss', loss_op)\n",
    "            tf.summary.scalar('accuracy', accuracy_op)\n",
    "            self.summaries = tf.summary.merge_all()\n",
    "        \n",
    "        saver = tf.train.Saver()\n",
    "        init = tf.global_variables_initializer()\n",
    "        \n",
    "        # Make the important operations available\n",
    "        self._X, self._y = X, y\n",
    "        self._Y_proba, self._loss_op = y_proba, loss_op\n",
    "        self._training_op, self._accuracy_op = training_op, accuracy_op\n",
    "        self._init, self._saver = init, saver\n",
    "        \n",
    "    def save(self, path):\n",
    "        self._saver.save(self.session, path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "hidden"
   },
   "source": [
    "Now we can use the RandomizedSearchCV class to search for the best hyperparameters. The number of iterations and the hyperparameter distributions can be tweaked depending on your computing resources:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "from scipy.stats import randint as sp_randint\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "\n",
    "def leaky_relu(alpha=0.01):\n",
    "    def parametrized_leaky_relu(z, name=None):\n",
    "        return tf.maximum(alpha * z, z, name=name)\n",
    "    return parametrized_leaky_relu\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "mnist = MnistData(min_digit=0, max_digit=4)\n",
    "X = np.concatenate((mnist.train_images, mnist.validation_images), axis=0)\n",
    "y = np.concatenate((mnist.train_labels, mnist.validation_labels), axis=0)\n",
    "\n",
    "# specify parameters and distributions to sample from\n",
    "param_dist = {\"optimizer\": [tf.train.GradientDescentOptimizer, tf.train.AdamOptimizer,\n",
    "                            tf.train.AdagradOptimizer, partial(tf.train.MomentumOptimizer, momentum=0.9)],\n",
    "              \"hidden_layers\": sp_randint(3, 8),\n",
    "              \"num_neurons\": sp_randint(50, 250),\n",
    "              \"batch_size\": sp_randint(20, 200),\n",
    "              \"activation\": [tf.nn.elu, leaky_relu(alpha=0.01), leaky_relu(alpha=0.1)],\n",
    "              \"learning_rate\": [1e-4, 3e-4, 1e-3, 3e-3, 1e-2],\n",
    "              \"initializer\": [he_init]\n",
    "             }\n",
    "\n",
    "# run randomized search\n",
    "n_iter_search = 15\n",
    "dnn = DNNClassifier()\n",
    "random_search = RandomizedSearchCV(dnn, param_distributions=param_dist,\n",
    "                                   n_iter=n_iter_search, verbose=2)\n",
    "\n",
    "random_search.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "y_pred = random_search.best_estimator_.predict(mnist.test_images)\n",
    "accuracy = accuracy_score(y_pred, mnist.test_labels)\n",
    "print('Accuracy of best classifier: {:.3f}'.format(accuracy * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "random_search.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "hidden"
   },
   "source": [
    "Now we can save the best model to disk, and restore it later when needed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "random_search.best_estimator_.save('./best_model_ex8.ckpt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "source": [
    "* d) Now try adding Batch Normalization and compare the learning curves: is it converging faster than before? Does it produce a better model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "dnn_with_bn = DNNClassifier(hidden_layers=5, activation=leaky_relu(alpha=0.01), initializer=he_init, \n",
    "                            num_neurons=191, optimizer=partial(tf.train.MomentumOptimizer, momentum=0.9),\n",
    "                            learning_rate=0.01, batch_size=42, batch_norm_momentum=0.9)\n",
    "dnn_with_bn.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "y_pred = dnn_with_bn.predict(mnist.test_images)\n",
    "accuracy = accuracy_score(y_pred, mnist.test_labels)\n",
    "print('Accuracy of best classifier with batch normalization: {:.3f}'.format(accuracy * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "hidden"
   },
   "source": [
    "If we use batch normalization the model will take lower to converge, slowing down training a bit. However, the performance of the model increased."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "source": [
    "* e) Is the model overfitting the training set? Try adding dropout to every layer and try again. Does it help?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "hidden"
   },
   "source": [
    "The model doesn't seem to be overfitting the training set (we even get a higher accuracy on the test set than on the validation set used for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "dnn_with_bn_and_dropout = DNNClassifier(hidden_layers=5, activation=leaky_relu(alpha=0.01), initializer=he_init, \n",
    "                            num_neurons=191, optimizer=partial(tf.train.MomentumOptimizer, momentum=0.9),\n",
    "                            learning_rate=0.01, batch_size=42, batch_norm_momentum=0.9, dropout_rate=0.5)\n",
    "dnn_with_bn_and_dropout.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "y_pred = dnn_with_bn_and_dropout.predict(mnist.test_images)\n",
    "accuracy = accuracy_score(y_pred, mnist.test_labels)\n",
    "print('Accuracy of best classifier with batch normalization and dropout: {:.3f}'.format(accuracy * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "hidden"
   },
   "source": [
    "In this case, using dropout hasn't increased the performance of our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "source": [
    "## Exercise 9\n",
    "\n",
    "Transfer learning:\n",
    "* Create a new DNN that reuses all the pretrained hidden layers of the previous model, freezes them, and replaces the softmax output layer with a new one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "saver = tf.train.import_meta_graph('best_model_ex8.ckpt.meta')\n",
    "\n",
    "graph = tf.get_default_graph()\n",
    "X = graph.get_tensor_by_name(\"x_input:0\")\n",
    "y = graph.get_tensor_by_name(\"y_input:0\")\n",
    "loss_op = graph.get_tensor_by_name(\"loss/loss:0\")\n",
    "accuracy_op = graph.get_tensor_by_name(\"accuracy/accuracy:0\")\n",
    "optimizer = tf.train.AdamOptimizer()\n",
    "train_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=\"logits\")\n",
    "training_op = optimizer.minimize(loss_op, var_list=train_vars)\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "source": [
    "* Train this new DNN on digits 5 to 9, using only 100 images per digit, and time how long it takes. Despite this small number of examples, can you achieve high precision?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "def sample_n_instances_per_class(X, y, n=100):\n",
    "    Xs, ys = [], []\n",
    "    for label in np.unique(y):\n",
    "        idx = (y == label)\n",
    "        Xc = X[idx][:n]\n",
    "        yc = y[idx][:n]\n",
    "        Xs.append(Xc)\n",
    "        ys.append(yc)\n",
    "    return np.concatenate(Xs), np.concatenate(ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "mnist_5_9 = MnistData(min_digit=5, max_digit=9)\n",
    "mnist_5_9.train_images, mnist_5_9.train_labels = sample_n_instances_per_class(mnist_5_9.train_images, \n",
    "                                                                              mnist_5_9.train_labels, 100)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, 'best_model_ex8.ckpt')\n",
    "    sess.run(init)\n",
    "\n",
    "    num_samples = 500\n",
    "    num_epochs = 1000\n",
    "    batch_size = 100\n",
    "    num_batches = num_samples // batch_size\n",
    "\n",
    "    MAX_CHECKS_NO_PROGRESS = 20\n",
    "    checks_no_progress = 0\n",
    "    best_loss = np.inf\n",
    "    for epoch in range(num_epochs):\n",
    "        for batch in range(num_batches):\n",
    "            # training step\n",
    "            X_batch, y_batch = fetch_batch(batch_size, batch, mnist_5_9.train_images, mnist_5_9.train_labels)\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "\n",
    "        loss, acc = sess.run([loss_op, accuracy_op], feed_dict={X: mnist_5_9.validation_images,\n",
    "                                                                y: mnist_5_9.validation_labels})\n",
    "        if loss < best_loss:\n",
    "            best_loss = loss\n",
    "            checks_no_progress = 0\n",
    "        else:\n",
    "            checks_no_progress += 1\n",
    "            if checks_no_progress >= MAX_CHECKS_NO_PROGRESS:\n",
    "                print(\"No progress after {} epochs. Stopping...\".format(epoch))\n",
    "                break\n",
    "        print(\"{}\\tValidation loss: {:.6f}\\tBest loss: {:.6f}\\tAccuracy: {:.3f}%\".format(\n",
    "            epoch, loss, best_loss, acc * 100))\n",
    "\n",
    "    acc_test = sess.run(accuracy_op, feed_dict={X: mnist_5_9.test_images, y: mnist_5_9.test_labels})\n",
    "    print(\"Final test accuracy: {:.3f}%\".format(acc_test * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "source": [
    "* Try caching the frozen layers, and train the model again: how much faster is it now?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "hidden5 = graph.get_tensor_by_name(\"hidden5_out:0\")\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, 'best_model_ex8.ckpt')\n",
    "    sess.run(init)\n",
    "    hidden5_cache = sess.run(hidden5, feed_dict={X: mnist_5_9.train_images, y: mnist_5_9.train_labels})\n",
    "    hidden5_val_cache = sess.run(hidden5, feed_dict={X: mnist_5_9.validation_images, y: mnist_5_9.validation_labels})\n",
    "\n",
    "    num_samples = 500\n",
    "    num_epochs = 1000\n",
    "    batch_size = 100\n",
    "    num_batches = num_samples // batch_size\n",
    "\n",
    "    MAX_CHECKS_NO_PROGRESS = 20\n",
    "    checks_no_progress = 0\n",
    "    best_loss = np.inf\n",
    "    for epoch in range(num_epochs):\n",
    "        for batch in range(num_batches):\n",
    "            # training step\n",
    "            hidden5_batch, y_batch = fetch_batch(batch_size, batch,  mnist_5_9.train_images, mnist_5_9.train_labels)\n",
    "            sess.run(training_op, feed_dict={X: hidden5_batch, y: y_batch})\n",
    "\n",
    "        loss, acc = sess.run([loss_op, accuracy_op], feed_dict={hidden5: hidden5_val_cache,\n",
    "                                                                y: mnist_5_9.validation_labels})\n",
    "        if loss < best_loss:\n",
    "            best_loss = loss\n",
    "            checks_no_progress = 0\n",
    "        else:\n",
    "            checks_no_progress += 1\n",
    "            if checks_no_progress >= MAX_CHECKS_NO_PROGRESS:\n",
    "                print(\"No progress after {} epochs. Stopping...\".format(epoch))\n",
    "                break\n",
    "        print(\"{}\\tValidation loss: {:.6f}\\tBest loss: {:.6f}\\tAccuracy: {:.3f}%\".format(\n",
    "            epoch, loss, best_loss, acc * 100))\n",
    "\n",
    "    acc_test = sess.run(accuracy_op, feed_dict={X: mnist_5_9.test_images, y: mnist_5_9.test_labels})\n",
    "    print(\"Final test accuracy: {:.3f}%\".format(acc_test * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "source": [
    "* Try again reusing just four hidden layers instead of five. Can you achieve a higher precision?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "saver = tf.train.import_meta_graph('best_model_ex8.ckpt.meta')\n",
    "n_outputs = 5\n",
    "\n",
    "graph = tf.get_default_graph()\n",
    "X = graph.get_tensor_by_name(\"x_input:0\")\n",
    "y = graph.get_tensor_by_name(\"y_input:0\")\n",
    "hidden4_out = graph.get_tensor_by_name(\"hidden4_out:0\")\n",
    "logits = tf.layers.dense(hidden4_out, n_outputs, kernel_initializer=he_init, name=\"new_logits\")\n",
    "loss_op = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits))\n",
    "softmax = tf.nn.softmax(logits)\n",
    "correct = tf.equal(tf.argmax(softmax, axis=1), y)\n",
    "accuracy_op = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer()\n",
    "train_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=\"new_logits\")\n",
    "training_op = optimizer.minimize(loss_op, var_list=train_vars)\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "mnist_5_9 = MnistData(min_digit=5, max_digit=9)\n",
    "mnist_5_9.train_images, mnist_5_9.train_labels = sample_n_instances_per_class(mnist_5_9.train_images, \n",
    "                                                                              mnist_5_9.train_labels, 100)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, 'best_model_ex8.ckpt')\n",
    "    sess.run(init)\n",
    "\n",
    "    num_samples = 500\n",
    "    num_epochs = 1000\n",
    "    batch_size = 100\n",
    "    num_batches = num_samples // batch_size\n",
    "\n",
    "    MAX_CHECKS_NO_PROGRESS = 20\n",
    "    checks_no_progress = 0\n",
    "    best_loss = np.inf\n",
    "    for epoch in range(num_epochs):\n",
    "        for batch in range(num_batches):\n",
    "            # training step\n",
    "            X_batch, y_batch = fetch_batch(batch_size, batch, mnist_5_9.train_images, mnist_5_9.train_labels)\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "\n",
    "        loss, acc = sess.run([loss_op, accuracy_op], feed_dict={X: mnist_5_9.validation_images,\n",
    "                                                                y: mnist_5_9.validation_labels})\n",
    "        if loss < best_loss:\n",
    "            best_loss = loss\n",
    "            checks_no_progress = 0\n",
    "        else:\n",
    "            checks_no_progress += 1\n",
    "            if checks_no_progress >= MAX_CHECKS_NO_PROGRESS:\n",
    "                print(\"No progress after {} epochs. Stopping...\".format(epoch))\n",
    "                break\n",
    "        print(\"{}\\tValidation loss: {:.6f}\\tBest loss: {:.6f}\\tAccuracy: {:.3f}%\".format(\n",
    "            epoch, loss, best_loss, acc * 100))\n",
    "\n",
    "    acc_test = sess.run(accuracy_op, feed_dict={X: mnist_5_9.test_images, y: mnist_5_9.test_labels})\n",
    "    print(\"Final test accuracy: {:.3f}%\".format(acc_test * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "source": [
    "* Now unfreeze the top two hidden layers and continue training: can you get the model to perform even better?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "saver = tf.train.import_meta_graph('best_model_ex8.ckpt.meta')\n",
    "n_outputs = 5\n",
    "\n",
    "graph = tf.get_default_graph()\n",
    "X = graph.get_tensor_by_name(\"x_input:0\")\n",
    "y = graph.get_tensor_by_name(\"y_input:0\")\n",
    "hidden4_out = graph.get_tensor_by_name(\"hidden4_out:0\")\n",
    "logits = tf.layers.dense(hidden4_out, n_outputs, kernel_initializer=he_init, name=\"new_logits\")\n",
    "loss_op = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits))\n",
    "softmax = tf.nn.softmax(logits)\n",
    "correct = tf.equal(tf.argmax(softmax, axis=1), y)\n",
    "accuracy_op = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer()\n",
    "train_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=\"hidden[34]|new_logits\")\n",
    "training_op = optimizer.minimize(loss_op, var_list=train_vars)\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "mnist_5_9 = MnistData(min_digit=5, max_digit=9)\n",
    "mnist_5_9.train_images, mnist_5_9.train_labels = sample_n_instances_per_class(mnist_5_9.train_images, \n",
    "                                                                              mnist_5_9.train_labels, 100)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, 'best_model_ex8.ckpt')\n",
    "    sess.run(init)\n",
    "\n",
    "    num_samples = 500\n",
    "    num_epochs = 1000\n",
    "    batch_size = 100\n",
    "    num_batches = num_samples // batch_size\n",
    "\n",
    "    MAX_CHECKS_NO_PROGRESS = 20\n",
    "    checks_no_progress = 0\n",
    "    best_loss = np.inf\n",
    "    for epoch in range(num_epochs):\n",
    "        for batch in range(num_batches):\n",
    "            # training step\n",
    "            X_batch, y_batch = fetch_batch(batch_size, batch, mnist_5_9.train_images, mnist_5_9.train_labels)\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "\n",
    "        loss, acc = sess.run([loss_op, accuracy_op], feed_dict={X: mnist_5_9.validation_images,\n",
    "                                                                y: mnist_5_9.validation_labels})\n",
    "        if loss < best_loss:\n",
    "            best_loss = loss\n",
    "            checks_no_progress = 0\n",
    "        else:\n",
    "            checks_no_progress += 1\n",
    "            if checks_no_progress >= MAX_CHECKS_NO_PROGRESS:\n",
    "                print(\"No progress after {} epochs. Stopping...\".format(epoch))\n",
    "                break\n",
    "        print(\"{}\\tValidation loss: {:.6f}\\tBest loss: {:.6f}\\tAccuracy: {:.3f}%\".format(\n",
    "            epoch, loss, best_loss, acc * 100))\n",
    "\n",
    "    acc_test = sess.run(accuracy_op, feed_dict={X: mnist_5_9.test_images, y: mnist_5_9.test_labels})\n",
    "    print(\"Final test accuracy: {:.3f}%\".format(acc_test * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "source": [
    "## Exercise 10\n",
    "\n",
    "Pretraining on an auxiliary task.\n",
    "* In this exercise you will build a DNN that compares two MNIST digit images and predicts whether they represent the same digit or not. Then you will reuse the lower layers of this network to train an MNIST classifier using very little training data. Start by building two DNNs (let's call them DNN A and B), both similar to the one you built earlier but without the output layer: each DNN should have five hidden layers of 100 neurons each, He initialization, and ELU activation. Next, add a single output layer on top of both DNNs. You should use TensorFlow's concat() function with axis=1 to concatenate the output of both DNNs along the horizontal axis, then feed the result to the output layer. This output layer should contain a single neuron using the logistic activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "def build_dnn_ex9(inputs, num_layers=5, num_neurons=100, name=\"dnn\"):\n",
    "    with tf.name_scope(name):\n",
    "        for i in range(num_layers):\n",
    "            inputs = tf.layers.dense(inputs, num_neurons, activation=tf.nn.elu,\n",
    "                                     kernel_initializer=he_init, name=\"hidden{}_{}\".format(i, name))        \n",
    "    return inputs\n",
    "\n",
    "tf.reset_default_graph()\n",
    "mnist = MnistData()\n",
    "num_inputs = np.shape(mnist.train_images)[1]\n",
    "X = tf.placeholder(tf.float32, shape=[None, 2, num_inputs], name=\"input_x\")\n",
    "X1, X2 = tf.unstack(X, axis=1)\n",
    "y = tf.placeholder(tf.float32, shape=[None, 1], name=\"input_y\")\n",
    "dnn_a = build_dnn_ex9(X1, name=\"dnn_a\")\n",
    "dnn_b = build_dnn_ex9(X2, name=\"dnn_b\")\n",
    "dnn_outputs = tf.concat([dnn_a, dnn_b], axis=1)\n",
    "hidden = tf.layers.dense(dnn_outputs, units=10, activation=tf.nn.elu, kernel_initializer=he_init)\n",
    "output = tf.layers.dense(hidden, units=1, activation=None, kernel_initializer=he_init, name=\"logits\")\n",
    "y_proba = tf.nn.sigmoid(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "source": [
    "* Split the MNIST training set in two sets: split #1 should contain 55,000 images, and split #2 should contain 5,000 images. Create a function that generates a training batch where each instance is a pair of MNIST images picked from split #1. Half of the training instances should be pairs of images that belong to the same class, while the other half should be images from different classes. For each pair, the training label should be 0 if the images are from the same class, or 1 if they are from different classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "hidden"
   },
   "source": [
    "The MnistData class already creates a training and validation set, with sizes 55,0000 and 5,000 respectively. So that part is already done. We will create the function that generates the training batch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "def generate_training_batch_ex9(X, y, batch_size):\n",
    "    newX, newY = [], []\n",
    "    \n",
    "    while len(newX) != batch_size // 2:\n",
    "        rnd_idx1, rnd_idx2 = np.random.randint(0, len(X), 2)\n",
    "        if rnd_idx1 == rnd_idx2:\n",
    "            continue\n",
    "            \n",
    "        if y[rnd_idx1] == y[rnd_idx2]:\n",
    "            newX.append(np.array([X[rnd_idx1], X[rnd_idx2]]))\n",
    "            newY.append([0])\n",
    "            \n",
    "    while len(newX) != batch_size:\n",
    "        rnd_idx1, rnd_idx2 = np.random.randint(0, len(X), 2)\n",
    "        if rnd_idx1 == rnd_idx2:\n",
    "            continue\n",
    "            \n",
    "        if y[rnd_idx1] != y[rnd_idx2]:\n",
    "            newX.append(np.array([X[rnd_idx1], X[rnd_idx2]]))\n",
    "            newY.append([1])\n",
    "    rnd_indices = np.random.permutation(batch_size)\n",
    "    return np.array(newX)[rnd_indices], np.array(newY)[rnd_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "source": [
    "* Train the DNN on this training set. For each image pair, you can simultaneously feed the first image to DNN A and the second image to DNN B. The whole network will gradually learn to tell whether two images belond to the same class or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "X_test, y_test = generate_training_batch_ex9(mnist.test_images, mnist.test_labels, len(mnist.test_images))\n",
    "\n",
    "with tf.name_scope('loss'):\n",
    "    loss_op = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=y, logits=output))\n",
    "\n",
    "with tf.name_scope('train'):\n",
    "    optimizer = tf.train.AdamOptimizer()\n",
    "    training_op = optimizer.minimize(loss_op)\n",
    "\n",
    "with tf.name_scope('accuracy'):\n",
    "    y_pred = tf.cast(tf.greater_equal(y_proba, 0.5), tf.float32)\n",
    "    correct = tf.equal(y_pred, y)\n",
    "    accuracy_op = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    num_samples = 500\n",
    "    num_epochs = 1000\n",
    "    batch_size = 100\n",
    "    num_batches = num_samples // batch_size\n",
    "\n",
    "    MAX_CHECKS_NO_PROGRESS = 20\n",
    "    checks_no_progress = 0\n",
    "    best_loss = np.inf\n",
    "    for epoch in range(num_epochs):\n",
    "        for batch in range(num_batches):\n",
    "            # training step\n",
    "            X_batch, y_batch = generate_training_batch_ex9(mnist.train_images, mnist.train_labels, batch_size)\n",
    "            _, loss_val = sess.run([training_op, loss_op], feed_dict={X: X_batch, y: y_batch})\n",
    "        if epoch % 5 == 0:\n",
    "            loss, acc = sess.run([loss_op, accuracy_op], feed_dict={X: X_test,\n",
    "                                                                    y: y_test})\n",
    "            print(\"{} Test loss: {:.6f} Test accuracy: {:.3f}%\".format(\n",
    "                  epoch, loss, acc * 100))\n",
    "\n",
    "    acc_test = sess.run(accuracy_op, feed_dict={X: X_test, y: y_test})\n",
    "    print(\"Final test accuracy: {:.3f}%\".format(acc_test * 100))\n",
    "    \n",
    "    save_path = saver.save(sess, \"./my_digit_comparison_model.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "source": [
    "* Now create a new DNN by reusing and freezing the hidden layers of DNN A, and adding a softmax output layer on with 10 neurons. Train this network on split #2 and see if you can achieve high performance despite having only 500 images per class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "n_inputs = 28 * 28  # MNIST\n",
    "n_outputs = 10\n",
    "\n",
    "restore_saver = tf.train.import_meta_graph('my_digit_comparison_model.ckpt.meta')\n",
    "\n",
    "graph = tf.get_default_graph()\n",
    "X = graph.get_tensor_by_name(\"input_x:0\")\n",
    "y = tf.placeholder(tf.int64, [None])\n",
    "last_hidden = graph.get_tensor_by_name(\"dnn_a/hidden4_dnn_a/Elu:0\")\n",
    "new_output = tf.layers.dense(last_hidden, units=10, activation=None, kernel_initializer=he_init, name=\"new_logits\")\n",
    "\n",
    "with tf.name_scope('loss'):\n",
    "    loss_op = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=new_output))\n",
    "\n",
    "with tf.name_scope('train'):\n",
    "    optimizer = tf.train.AdamOptimizer()\n",
    "    train_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=\"new_logits\")\n",
    "    training_op = optimizer.minimize(loss_op, var_list=train_vars)\n",
    "\n",
    "with tf.name_scope('accuracy'):\n",
    "    y_proba = tf.nn.softmax(new_output, name=\"y_proba\")\n",
    "    correct = tf.equal(tf.argmax(y_proba, axis=1), y)\n",
    "    accuracy_op = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "n_epochs = 100\n",
    "batch_size = 50\n",
    "\n",
    "mnist = MnistData()\n",
    "X_test, y_test = mnist.test_images, mnist.test_labels\n",
    "X_train2, y_train2 = mnist.validation_images, mnist.validation_labels\n",
    "X_train2 = np.stack((X_train2, np.zeros([5000, 784])), axis=1)\n",
    "X_test = np.stack((X_test, np.zeros([10000, 784])), axis=1)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    restore_saver.restore(sess, \"./my_digit_comparison_model.ckpt\")\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        rnd_idx = np.random.permutation(len(X_train2))\n",
    "        for rnd_indices in np.array_split(rnd_idx, len(X_train2) // batch_size):\n",
    "            X_batch, y_batch = X_train2[rnd_indices], y_train2[rnd_indices]\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        if epoch % 10 == 0:\n",
    "            acc_test = accuracy_op.eval(feed_dict={X: X_test, y: y_test})\n",
    "            print(epoch, \"Test accuracy:\", acc_test)\n",
    "\n",
    "    save_path = saver.save(sess, \"./my_mnist_model_final.ckpt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
