{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 13: Convolutional Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1\n",
    "What are the advantages of a CNN over a fully connected DNN for image classification?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, CNN's require a lot less parameters than fully connected DNN's. When working with images this can be really noticeable: imagine we have 800x600 pixels images as input, with 3 channels each (RGB). With a fully connected network with a hidden layer of 1000 neurons we will have $800*600*3*1000 = 1.44 * 10^9$ weights to tweak. On the other hand, a CNN with a convolutional layer of 1000 feature maps and a kernel of size 5x5 will have only $5*5*3*1000 = 7.5 * 10^4$ weights.\n",
    "Another advantage of CNN's is that each neuron only is influenced by some neighbouring neurons (receptive field) of the previous layer. This allow the network to concentrate on low-level features on the lower layers, and combine them into higher level features in the following layers. This works really well with real world images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2\n",
    "Consider a CNN composed of three convolutional layers, each with 3x3 kernels, a stride of 2 and same padding. The lowest layer outputs 100 feature maps, the middle one outputs 200, and the top one outputs 400. The input images are RGB images of 200x300 pixels. What is the total number of parameters in the CNN? If we are using 32-bit floats, at least how much RAM will this network require when making a prediction for a single instance? What about when training on a mini-batch of 50 images?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with the total number of parameters. The first layer will have $(3*3*3+1)*100 = 2800$ parameters, the second $(3*3*100+1)*200 = 180200$ and the third $(3*3*200+1)*400 = 720400$, so the total number of parameters will be $2800 + 180200 + 720400 = 903400$<br>\n",
    "If we are using 32-bit floats, the network will require $((100*100*150) + (200*50*75) + (400*25*37)) * 32 = 83.84 * 10^6$ bits of RAM (around 10MB). If we trin a minibatch of 50 images, it will require 500MB or RAM (10 * 50)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3\n",
    "If your GPU runs out of memory while training a CNN, what are five things you could try to solve the problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ol>\n",
    "    <li>You can use a different data type that requires less bits to store the data (e.g. if you are using 32 bit floats to store each neuron values, try using 16 bit floats).</li>\n",
    "    <li>You can try to reduce the mini-batch size.</li>\n",
    "    <li>Another solution could be to use strides to reduce the amount of data that reaches higher layers.</li>\n",
    "    <li>You can also remove some layers</li>\n",
    "    <li>There is also the possibility of distributing the network across multiple devices.</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4\n",
    "Why would you want to add a max pooling layer rather than a convolutional layer with the same stride?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Max pooling layers are made to reduce the computational load of the network by shrinking the input they receive. They don't have weights like convolutional layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5\n",
    "When would you want to add a <i>local response normalization layer</i>?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to improve generalization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 6\n",
    "Can you name the main innovations in AlexNet, compared to LeNet-5? What about the main innovations in GoogLeNet and ResNet?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main innovations in AlexNet were to concatenate multiple convolutional layers in top of each other (in LeNet-5 each convolutional layer was followed by a pooling layer), adding local response normalization to some of the convolutional layers, and dropout to the fully connected layers.<br>\n",
    "The main innovations in GoogLeNet were creating a much deeper CNN thanks to the inception modules, and adding convolutional layers with a kernel size of 1x1 to serve as bottleneck layers.<br>\n",
    "Finally, ResNet added skip connections and residual leraning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 7\n",
    "Build your own CNN and try to achieve the highest possible accuracy on MNIST."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from MnistData import MnistData\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "mnist = MnistData.get_LeNet_data(one_hot=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if the mnist data prepared for the LeNet architecture looks good (it is padded with 0's to have 32x32 pixels)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def visualize(digit, img_width, img_height):\n",
    "    digit2d = np.reshape(digit, (img_width, img_height))\n",
    "    plt.imshow(digit2d, cmap='summer')\n",
    "    \n",
    "visualize(mnist.train.images[0], 32, 32)\n",
    "mnist.train.labels[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks good :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 - Loss: 0.107649 - Best loss: 0.1076492 - Accuracy: 0.967\n",
      "1 - Loss: 0.075342 - Best loss: 0.0753418 - Accuracy: 0.977\n",
      "2 - Loss: 0.064524 - Best loss: 0.0645238 - Accuracy: 0.980\n",
      "3 - Loss: 0.057055 - Best loss: 0.0570546 - Accuracy: 0.982\n",
      "4 - Loss: 0.052431 - Best loss: 0.0524311 - Accuracy: 0.985\n",
      "5 - Loss: 0.047797 - Best loss: 0.0477969 - Accuracy: 0.985\n",
      "6 - Loss: 0.048230 - Best loss: 0.0477969 - Accuracy: 0.987\n",
      "7 - Loss: 0.047138 - Best loss: 0.0471382 - Accuracy: 0.987\n",
      "8 - Loss: 0.049032 - Best loss: 0.0471382 - Accuracy: 0.987\n",
      "9 - Loss: 0.053635 - Best loss: 0.0471382 - Accuracy: 0.987\n",
      "10 - Loss: 0.050232 - Best loss: 0.0471382 - Accuracy: 0.988\n",
      "11 - Loss: 0.042459 - Best loss: 0.0424588 - Accuracy: 0.990\n",
      "12 - Loss: 0.050992 - Best loss: 0.0424588 - Accuracy: 0.987\n",
      "13 - Loss: 0.056658 - Best loss: 0.0424588 - Accuracy: 0.986\n",
      "14 - Loss: 0.048618 - Best loss: 0.0424588 - Accuracy: 0.987\n",
      "15 - Loss: 0.039781 - Best loss: 0.0397815 - Accuracy: 0.990\n",
      "16 - Loss: 0.049549 - Best loss: 0.0397815 - Accuracy: 0.988\n",
      "17 - Loss: 0.045780 - Best loss: 0.0397815 - Accuracy: 0.989\n",
      "18 - Loss: 0.052491 - Best loss: 0.0397815 - Accuracy: 0.988\n",
      "19 - Loss: 0.048068 - Best loss: 0.0397815 - Accuracy: 0.989\n",
      "20 - Loss: 0.041270 - Best loss: 0.0397815 - Accuracy: 0.991\n",
      "21 - Loss: 0.048875 - Best loss: 0.0397815 - Accuracy: 0.989\n",
      "22 - Loss: 0.047602 - Best loss: 0.0397815 - Accuracy: 0.988\n",
      "23 - Loss: 0.041445 - Best loss: 0.0397815 - Accuracy: 0.991\n",
      "24 - Loss: 0.044478 - Best loss: 0.0397815 - Accuracy: 0.991\n",
      "25 - Loss: 0.047727 - Best loss: 0.0397815 - Accuracy: 0.991\n",
      "26 - Loss: 0.044846 - Best loss: 0.0397815 - Accuracy: 0.991\n",
      "27 - Loss: 0.051091 - Best loss: 0.0397815 - Accuracy: 0.992\n",
      "28 - Loss: 0.071906 - Best loss: 0.0397815 - Accuracy: 0.986\n",
      "29 - Loss: 0.041887 - Best loss: 0.0397815 - Accuracy: 0.991\n",
      "30 - Loss: 0.046641 - Best loss: 0.0397815 - Accuracy: 0.991\n",
      "31 - Loss: 0.053190 - Best loss: 0.0397815 - Accuracy: 0.989\n",
      "32 - Loss: 0.051509 - Best loss: 0.0397815 - Accuracy: 0.990\n",
      "33 - Loss: 0.052846 - Best loss: 0.0397815 - Accuracy: 0.988\n",
      "34 - Loss: 0.048065 - Best loss: 0.0397815 - Accuracy: 0.992\n",
      "No progress after 20 epochs. Stopping...\n"
     ]
    }
   ],
   "source": [
    "from LeNet import LeNet\n",
    "\n",
    "leNet = LeNet()\n",
    "leNet.fit(mnist.train.images, mnist.train.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final accuracy: 98.920\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "y_pred = leNet.predict(mnist.test.images)\n",
    "test_accuracy = accuracy_score(y_pred, mnist.test.labels)\n",
    "print('Final accuracy: {:.3f}'.format(test_accuracy * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 8\n",
    "Classifying large images using Inception v3:\n",
    "* Download some images of various animals. Load them in Python, for example using the <i>matplotlib.image.pmimg.imread()</i> function or the <i>scipy.misc.imread()</i> function. Resize and/or crop them to 299x299 picels and ensure that they have just three channels (RGB), with no transparency channel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Download the latest pretrained Inception v3 model: the checkpoint is available at https://goo.gl/nxSQvl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Create the Inception v3 model by calling the <i>inception_v3()</i> function, as shown below. This must be done within an argument scope created by the <i>inception_v3_arg_scope()</i> function. Also, you must set <i>is_training=False</i> and <i>num_classes=1001</i>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Open a session and use the Saver to restore the pretrained model checkpoint you downloaded earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Run the model to classify the images you prepared. Display the top five predictions for each image, along with the estimated probability (the list of class names is available at https://goo.gl/brXRtZ). How accurate is the model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 9\n",
    "Transfer learning for large image classification.\n",
    "* Create a trining set containing at least 100 images per class. For example, you could classify your own pictures based on the location (beach, mountain, city, etc.), opr alternatively you can just use an existing dataset, such as the flowers dataset (https://goo.gl/EgJVXZ) or MIT's places dataset (http://places.csail.mit.edu/) (requires registration, and it is huge)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Write a preprocessing step that will resize and crop the image to 299 x 299, with some randomness for data augmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Using the pretrained Inception v3 model from the previous exercise, freeze all layers up to the bottleneck layer (i.e., the last layer before the output layer), and replace the output layer with the appropriate number of outputs for your new classification task (e.g., the flowers dataset has five mutually exclusive classes so the output layer must have five neurons and use the softmax activation function)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Split your dataset into training set and test set. Train the model on the training set and evaluate it on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 10\n",
    "Go through TensorFlow's DeepDream tutorial (https://goo.gl/4b2s6g). It is a fun way to familiarize yourself with various ways of visualizing the patterns learned by a CNN, and to generate art using Deep Learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
