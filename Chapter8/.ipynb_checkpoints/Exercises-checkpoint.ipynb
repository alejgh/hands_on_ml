{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 8: Dimensaionality Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1\n",
    "What are the main motivations for reducing a dataset's dimensionality? What are the main drawbacks?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you reduce the dimensionality of a dataset you will reduce the memory and time required to train a model. If the dimension is low enough you can even visualize the dataset and analyze the results obtained more easily.\n",
    "The main drawback of reducing the dimensionality of a dataset is that you can end up losing some amount of information about the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2\n",
    "What is the curse of dimensionality?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Higher dimensional datasets tend to be more sparse, so every instance ends up far away from other instances. The result of this is that models trained on high dimensional datasets tend to overfit the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3\n",
    "Once a dataset's dimensionality has been reduced, is it possible to reverse the operation? If so, how? If not, why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some algorithms (e.g. PCA) have a reverse transformation to obtain the original dataset. However, as discussed earlier, when we reduce the dimensionality of a dataset we also lose some information, so even after applying the reverse transformation we will not get exactly the original data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4\n",
    "Can PCA be used to reduce the dimensionality of a highly nonlinear dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA can be use to reduce the dimensionality of most datasets. However, if there are no useless dimensions (e.g. Swiss roll), then reducing dimensionality with PCA will lose too much information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5\n",
    "Suppose you perform PCA on a 1,000-dimensional dataset, setting the explained variance ratio to 95%. How many dimensions will the resulting dataset have?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It depends on the dataset. We can reduce some datasets to more dimensions than others while retaining the same variance ratio."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 6\n",
    "In what cases would you use vanilla PCA, Incremental PCA, Randomized PCA or Kernel PCA?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For vanilla PCA we need the dataset to fit in memory, but it should be the default choice. I would use incremental PCA when working with a huge dataset that doesn't fit in memory or for online tasks (we need to apply PCA on the fly, every time we get a new instance). Randomized PCA is faster than vanilla PCA, but it also requires the data to fit in memory. Finally, kernel PCA should be used with nonlinear datasets. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 7\n",
    "How can you evaluate the performance of a dimensionality reduction algorithm on your dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have two options to measure the performance of a dimensionality reduction algorithm:\n",
    "* After reducing the dimensionality of a dataset we can apply the reverse transformation to get the original data and see how much information we have lost.\n",
    "* We can create a pipe where we have the dimensionality reduction algorithm and later on the model we want to train with the reduced data. Doing this we can try several algorithms and check which one performs best taking into account the performance of the final model, which uses the reduced data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 8\n",
    "Does it make any sense to chain two different dimensionality reduction algorithms?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes, we can chain two different dimensionality reduction algorithms. For example, we can use PCA to quickly get rid of a large number of dimensions, and then apply another much slower dimensionality reduction algorithm (e.g. LLE) for the final dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 9\n",
    "Load the MNIST dataset and split it into a training set and a test set (take the first 60,000 instances for training, and the remaining 10,000 for testing). Train a Random Forest classifier on the dataset and time how long it takes, then evaluate the resulting model on the test set. Next, use PCA to reduce the dataset's dimensionality, with an explained variance ratio of 95%. Train a new Random Forest classifier on the reduced dataset and see how long it takes. Was training much faster? Next evaluate the classifier on the test set: how does it compare to the previous classifier?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_mldata\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "digits = fetch_mldata('MNIST original')\n",
    "X, y = digits['data'], digits['target']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=60000, test_size=10000, random_state=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the MNIST data ready, we will train a Random Forest on it. We will use a default random forest, with no hyperparameter tuning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "random_forest_clf = RandomForestClassifier()\n",
    "%timeit random_forest_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can evaluate the model on the test data. We will just use accuracy as a measure of performance (it would be better to use other measures such as precision or recall for this dataset):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "y_pred = random_forest_clf.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print('Accuracy of random forest on original dataset: {:.2f}'.format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, we get around 95% accuracy using the original dataset. Now we will use PCA to reduce the dimension of the dataset, with a variance ratio of 95%:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# if n_components is a float, it specifies the variance ratio.\n",
    "# if it is an int, it specifies the number of dimensions to keep\n",
    "pca = PCA(n_components=0.95)\n",
    "pca.fit(X)\n",
    "X_train_reduced = pca.transform(X_train)\n",
    "X_test_reduced = pca.transform(X_test)\n",
    "print('Number of dimensions after applying PCA: {}'.format(len(X_train_reduced[0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will train the random forest on the reduced dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forest_clf = RandomForestClassifier()\n",
    "%timeit random_forest_clf.fit(X_train_reduced, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = random_forest_clf.predict(X_test_reduced)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print('Accuracy of random forest on the reduced dataset: {:.2f}'.format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 10\n",
    "Use t-SNE to reduce the MNIST dataset down to two dimensions and plot the result using Matplotlib. You can use a scatterplot using 10 different colors to represent each image's target class. Alternatively, you can write colored digits at the location of each instance, or even plot scaled-down versions of the digit images themselves. You should get a nice visualization with well-separated clusters of digits. Try using other dimensionality reduction algorithms such as PCA, LLE, or MDS and compare the resulting visualizations."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
