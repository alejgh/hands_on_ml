{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 9: Up and Running with TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1\n",
    "What are the main benefits of creating a computation graph rather than directly executing the computations? What are the main drawbacks?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Benefits:\n",
    "* You can break up the graph in several parts and run them in parallel across different CPUs or GPUs. You could even distribute them across different servers, you can train colossal neural networks in a reasonable amount of time. \n",
    "* TensorFlow can automatically compute the gradients fro you.\n",
    "* It simplifies introspection (e.g. using TensorBoard).\n",
    "\n",
    "Drawbacks:\n",
    "* It is more difficult to learn.\n",
    "* It makes step-by-step debugging harder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2\n",
    "Is the statement a_val = a.eval(session=sess) equivalent to a_val = sess.run(a)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes, they are equivalent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3\n",
    "Is the statement a_val, b_val = a.eval(session=sess), b.eval(session=sess) equivalent to a_val, b_val = sess.run([a, b])?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No, they are not the same. The first statement runs the graph twice (one time to compute a_val, and another time to compute b_val), while the second statement runs the graph once to compute both values. If any of these operations (or the operations they depend on) has side effects, the result could be different."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4\n",
    "Can you run two graphs in the same session?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No, you can't run two graphs in the same session."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5\n",
    "If you create a graph g containing a variable w, then start two threads and open a session in each thread, both using the same graph g, will each session have its own copy of the variable w or will it be shared?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, each thread will have its own copy of the variable w. However, in distributed TensorFlow they will share the same variable w."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 6\n",
    "When is a variable initialized? When is it destroyed?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A variable is initialized when you call its initializer, and it is destroyed when its session ends."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 7\n",
    "What is the difference between a placeholder and a variable?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A placeholder allows you to define a graph that depends on some value that will be later on passed to it in the execution phase. It is usually used to pass input data to the graph.\n",
    "A variable is pretty similar to the usual programming variable. It has a value from the start, and you can assign new values to it both in the construction phase (with tf.assign) and in the execution phase (with the assigment operator)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 8\n",
    "What happens when you run the graph to evaluate an operation that depends on a placeholder but you don't feed its value? What happens if the operation does not depend on the placeholder?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "tf.reset_default_graph()\n",
    "x = tf.placeholder(tf.float32, None)\n",
    "y = x + 2\n",
    "with tf.Session() as sess:\n",
    "    try:\n",
    "        result = y.eval()\n",
    "    except Exception:\n",
    "        print(\"Caught an exception!\")\n",
    "    else:\n",
    "        print(\"No exception caught: result was {}\".format(result))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "x = tf.placeholder(tf.float32, None)\n",
    "y = tf.Variable(5, name=\"y\")\n",
    "z = y * 2\n",
    "with tf.Session() as sess:\n",
    "    try:\n",
    "        y.initializer.run()\n",
    "        result = z.eval()\n",
    "    except Exception:\n",
    "        print(\"Caught an exception!\")\n",
    "    else:\n",
    "        print(\"No exception caught: result was {}\".format(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 9\n",
    "When you run a graph, can you feed the output value of any operation, or just the value of placeholders?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can feed the output value of any operation. However, it is pretty rare to feed the output value of operations other than placeholders."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 10\n",
    "How can you set a variable to any value you want (during the execution phase)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can create an assignment node during the construction phase, so later on during the execution phase the value of the variable specified in the assignment node will be changed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 11\n",
    "How many times does reverse-mode autodiff need to traverse the graph in order to compute the gradients of the cost function with regards to 10 variables? What about forward-mode autodiff? And symbolic differentiation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reverse-mode autodiff need to traverse the graph only two times to compute the gradients of the cost function, regardless of the number of variables.<br>\n",
    "Forward-mode autodiff would need to run once for each variable, so it would run 10 times in this case.<br>\n",
    "Symbolic differentiation would build a different graph to compute the gradients, so it would not traverse the original graph a single time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 12\n",
    "Implement Logistic Regression with Mini-batch Gradient Descent using TensorFlow. Train it and evaluate it on the moons dataset. Try adding all the bells and whistles:\n",
    "* Define the graph within a logistic_regression() function that can be reused easily.\n",
    "* Save checkpoints using a Saver at regular intervals during training, and save the final model at the end of training.\n",
    "* Restore the last checkpoint upon startup if training was interrupted.\n",
    "* Define the graph using nice scopes so the graph looks good in TensorBoard.\n",
    "* Add summaries to visualize the learning curves in TensorBoard.\n",
    "* Try tweaking some hyperparameters such as the learning rate or the mini-batch size and look at the shape of the learning curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "NUM_SAMPLES = 1000\n",
    "X, y = make_moons(NUM_SAMPLES, noise=0.1, random_state=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from logistic_regressor import TFLogisticRegressor\n",
    "\n",
    "log_reg = TFLogisticRegressor()\n",
    "log_reg.fit(X, y)\n",
    "y_pred = log_reg.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.867\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy = accuracy_score(y_pred, y)\n",
    "print('Accuracy: {:.3f}'.format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot decision boundary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "xx, yy = np.mgrid[-3:3:.01, -3:3:.01]\n",
    "grid = np.c_[xx.ravel(), yy.ravel()]\n",
    "probs = log_reg.predict_proba(grid)[:].reshape(xx.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(-3, 3), Text(0,0.5,'$X_2$'), (-3, 3), Text(0.5,0,'$X_1$'), None]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "f, ax = plt.subplots(figsize=(8, 6))\n",
    "contour = ax.contourf(xx, yy, probs, 20, cmap=\"RdBu\",\n",
    "                      vmin=0, vmax=1)\n",
    "ax_c = f.colorbar(contour)\n",
    "ax_c.set_label(\"$P(y = 1)$\")\n",
    "ax_c.set_ticks([0, .25, .5, .75, 1])\n",
    "\n",
    "ax.scatter(X[300:,0], X[300:, 1], c=y[300:], s=50,\n",
    "           cmap=\"RdBu\", vmin=-.2, vmax=1.2,\n",
    "           edgecolor=\"white\", linewidth=1)\n",
    "\n",
    "ax.set(aspect=\"equal\",\n",
    "       xlim=(-3, 3), ylim=(-3, 3),\n",
    "       xlabel=\"$X_1$\", ylabel=\"$X_2$\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=(8, 6))\n",
    "contour = ax.contourf(xx, yy, probs, levels=[.5, .501], vmin=0, vmax=.6)\n",
    "\n",
    "ax.scatter(X[300:,0], X[300:, 1], c=y[300:], s=50,\n",
    "           cmap=\"RdBu\", vmin=-.2, vmax=1.2,\n",
    "           edgecolor=\"white\", linewidth=1)\n",
    "\n",
    "ax.set(aspect=\"equal\",\n",
    "       xlim=(-3, 3), ylim=(-3, 3),\n",
    "       xlabel=\"$X_1$\", ylabel=\"$X_2$\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
