{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 10: Introduction to Artificial Neural Networks\n",
    "## Exercise 1\n",
    "Draw an ANN using the original artificial neurons that computes A XOR B."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![solution](img/xor.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2\n",
    "Why is it generally preferable to use a Logistic Regression classifier rather than a classical Perceptron? How can you tweak a perceptron to make it equivalent to a Logistic Regression classifier?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is preferable to use a Logistic Regression classifier because you will also obtained the probability of each predicted class.\n",
    "However, you can change the activation function to the logistic activation function to make the perceptron equivalent to a Logistic Regression classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3\n",
    "Why was the logistic activation function a key ingredient in training the first MLPs?\n",
    "The previous step function contained only flat segments, so Gradient Descent couldn't work with it. However, it will work with the logistic activation function, so it was possible to apply "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The previous step function contained only flat segments, so Gradient Descent couldn't work with it. However, it will work with the logistic activation function, so it was possible to apply better training techniques (i.e. backpropagation)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4\n",
    "Name three popular activation functions. Can you draw them?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Step function\n",
    "* Logistic function\n",
    "* ReLU function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5\n",
    "Suppose you have an MLP composed of one input layer with 10 passthrough neurons, followed b y one hidden layer with 50 artificial neurons, and finally one output layer with 3 artificial neurons. All artificial neurons use the ReLU activation function.\n",
    "* What is the shape of the input matrix $X$?<br>\n",
    "The input matrix X will have n rows (as many as training instances) and 10 columns (number of passthrough neurons).<br><br>\n",
    "* What about the shape of the hidden layer's weight vector $W_h$, and its bias vector $b_h$?<br>\n",
    "$W_h$ will have 10 rows and 50 columns. $b_h$ will have a length of 50.<br><br>\n",
    "* What is the shape of the output layer's weight vector $W_o$, and its bias vector $b_o$?<br>\n",
    "$W_o$ will have 50 rows and 3 columns. $b_o$ will have a length of 3.<br><br>\n",
    "* What is the shape of the output matrix $Y$?<br>\n",
    "n rows and 3 columns<br><br>\n",
    "* Write the equation that computes the network's output matrix Y as a function of $X$, $W_h$, $b_h$, $W_o$ and $b_o$.<br>\n",
    "$Y = ReLU(ReLU(X * W_h + b_h) * W_o + b_o)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 6\n",
    "How many neurons do you need in the output layer if you want to classify email into spam or ham? What activation function should you use in the output layer? If instead you want to tackle MNIST, how many neurons do you need in the output layer, using what activation function? Answer the same questions for getting your network to predict housing prices as in Chapter 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You need 1 neuron to classify email into spam or ham, and you should use the step activation function.<br>\n",
    "If you want to tackle MNIST, you need 10 neurons in the output layer (1 for each digit), and use the softmax activation function to get the probability of each digit for the input image. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 7\n",
    "What is the backpropagation and how does it work? What is the difference between backpropagation and reverse-mode autodiff?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Backpropagation is a technique used to train artificial neural netowrks. It first computes the gradients of the cost function with regards to every model parameter (all the weights and biases), and then it performs a Gradient Descent step using these gradients. This backpropagation step is typically performed thousands or millions of times, using many training batches, until the model parameters converge to values that minimize the cost function. To compute the gradients, backpropagation uses reverse-mode autodiff. Reverse-mode autodiff performs a forward pass through a computation graph, computing every node's value for the current training batch, and then it performs a reverse pass, computing all the gradients at once."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 8\n",
    "Can you list all the hyperparameters you can tweak in an MLP? If the MLP overfits the training data, how could you tweak these hyperparameters to try to solve the problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of hidden layers, number of neurons in each hidden layer, the activation function used in each hidden layer and in the output layer.<br>If the MLP overfits the training data, you can try reducing the number of hidden layers and reducing the number of neurons per hidden layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 9\n",
    "Train a deep MLP on the MNIST dataset and see if you can get over 98% precision. Just like in the last exercise of Chapter 9, try adding all the bells and whistles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "Num Inputs: 55000\n",
      "Num classes: 10\n",
      "Batch size: 100\n",
      "Epoch: 0001 cost=0.722978314 acc=0.788490910\n",
      "Epoch: 0002 cost=0.260589027 acc=0.927218185\n",
      "Epoch: 0003 cost=0.197462743 acc=0.945290913\n",
      "Epoch: 0004 cost=0.162719758 acc=0.954709095\n",
      "Epoch: 0005 cost=0.138475280 acc=0.962145459\n",
      "Epoch: 0006 cost=0.120020849 acc=0.967472733\n",
      "Epoch: 0007 cost=0.105181474 acc=0.971909099\n",
      "Epoch: 0008 cost=0.092896690 acc=0.975200009\n",
      "Epoch: 0009 cost=0.082565782 acc=0.978327282\n",
      "Epoch: 0010 cost=0.073746012 acc=0.980963646\n",
      "Epoch: 0011 cost=0.066004219 acc=0.982890919\n",
      "Epoch: 0012 cost=0.059205823 acc=0.984636374\n",
      "Epoch: 0013 cost=0.053196895 acc=0.986563646\n",
      "Epoch: 0014 cost=0.047834546 acc=0.988254555\n",
      "Epoch: 0015 cost=0.043023837 acc=0.989454554\n",
      "Epoch: 0016 cost=0.038718862 acc=0.990945462\n",
      "Epoch: 0017 cost=0.034811583 acc=0.992200007\n",
      "Epoch: 0018 cost=0.031281397 acc=0.993345461\n",
      "Epoch: 0019 cost=0.028044420 acc=0.994363642\n",
      "Epoch: 0020 cost=0.025129614 acc=0.995400004\n"
     ]
    }
   ],
   "source": [
    "from multilayer_perceptron import MultilayerPerceptron\n",
    "\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "n_inputs = len(mnist.train.images)\n",
    "n_classes = len(mnist.train.labels[0])\n",
    "batch_size = 100\n",
    "\n",
    "print('Num Inputs: {}'.format(n_inputs))\n",
    "print('Num classes: {}'.format(n_classes))\n",
    "print('Batch size: {}'.format(batch_size))\n",
    "\n",
    "mlp = MultilayerPerceptron(n_inputs, n_classes, batch_size)\n",
    "mlp.add_layer(300, activation=tf.nn.relu, name=\"hidden_layer\")\n",
    "mlp.add_layer(100, activation=tf.nn.relu, name=\"hidden_layer_2\")\n",
    "mlp.add_layer(10, name=\"output_layer\")\n",
    "mlp.fit(mnist.train.images, mnist.train.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(y_pred, y_true):\n",
    "    assert np.shape(y_pred) == np.shape(y_true)\n",
    "    \n",
    "    correct_predictions = np.sum(y_pred == y_true)\n",
    "    return correct_predictions / np.shape(y_pred)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.9731\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "model_pred = mlp.predict(mnist.test.images)\n",
    "true_pred = np.argmax(mnist.test.labels, axis=1)\n",
    "\n",
    "accuracy = get_accuracy(model_pred, true_pred)\n",
    "print('Accuracy: ', accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
