{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 10: Introduction to Artificial Neural Networks\n",
    "## Exercise 1\n",
    "Draw an ANN using the original artificial neurons that computes A XOR B."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![solution](img/xor.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2\n",
    "Why is it generally preferable to use a Logistic Regression classifier rather than a classical Perceptron? How can you tweak a perceptron to make it equivalent to a Logistic Regression classifier?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is preferable to use a Logistic Regression classifier because you will also obtained the probability of each predicted class.\n",
    "However, you can change the activation function to the logistic activation function to make the perceptron equivalent to a Logistic Regression classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3\n",
    "Why was the logistic activation function a key ingredient in training the first MLPs?\n",
    "The previous step function contained only flat segments, so Gradient Descent couldn't work with it. However, it will work with the logistic activation function, so it was possible to apply "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The previous step function contained only flat segments, so Gradient Descent couldn't work with it. However, it will work with the logistic activation function, so it was possible to apply better training techniques (i.e. backpropagation)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4\n",
    "Name three popular activation functions. Can you draw them?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Step function\n",
    "* Logistic function\n",
    "* ReLU function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5\n",
    "Suppose you have an MLP composed of one input layer with 10 passthrough neurons, followed b y one hidden layer with 50 artificial neurons, and finally one output layer with 3 artificial neurons. All artificial neurons use the ReLU activation function.\n",
    "* What is the shape of the input matrix $X$?<br>\n",
    "The input matrix X will have n rows (as many as training instances) and 10 columns (number of passthrough neurons).<br><br>\n",
    "* What about the shape of the hidden layer's weight vector $W_h$, and its bias vector $b_h$?<br>\n",
    "$W_h$ will have 10 rows and 50 columns. $b_h$ will have a length of 50.<br><br>\n",
    "* What is the shape of the output layer's weight vector $W_o$, and its bias vector $b_o$?<br>\n",
    "$W_o$ will have 50 rows and 3 columns. $b_o$ will have a length of 3.<br><br>\n",
    "* What is the shape of the output matrix $Y$?<br>\n",
    "n rows and 3 columns<br><br>\n",
    "* Write the equation that computes the network's output matrix Y as a function of $X$, $W_h$, $b_h$, $W_o$ and $b_o$.<br>\n",
    "$Y = ReLU(ReLU(X * W_h + b_h) * W_o + b_o)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 6\n",
    "How many neurons do you need in the output layer if you want to classify email into spam or ham? What activation function should you use in the output layer? If instead you want to tackle MNIST, how many neurons do you need in the output layer, using what activation function? Answer the same questions for getting your network to predict housing prices as in Chapter 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You need 1 neuron to classify email into spam or ham, and you should use the step activation function.<br>\n",
    "If you want to tackle MNIST, you need 10 neurons in the output layer (1 for each digit), and use the softmax activation function to get the probability of each digit for the input image. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 7\n",
    "What is the backpropagation and how does it work? What is the difference between backpropagation and reverse-mode autodiff?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Backpropagation is a technique used to train artificial neural netowrks. It first computes the gradients of the cost function with regards to every model parameter (all the weights and biases), and then it performs a Gradient Descent step using these gradients. This backpropagation step is typically performed thousands or millions of times, using many training batches, until the model parameters converge to values that minimize the cost function. To compute the gradients, backpropagation uses reverse-mode autodiff. Reverse-mode autodiff performs a forward pass through a computation graph, computing every node's value for the current training batch, and then it performs a reverse pass, computing all the gradients at once."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 8\n",
    "Can you list all the hyperparameters you can tweak in an MLP? If the MLP overfits the training data, how could you tweak these hyperparameters to try to solve the problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of hidden layers, number of neurons in each hidden layer, the activation function used in each hidden layer and in the output layer.<br>If the MLP overfits the training data, you can try reducing the number of hidden layers and reducing the number of neurons per hidden layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 9\n",
    "Train a deep MLP on the MNIST dataset and see if you can get over 98% precision. Just like in the last exercise of Chapter 9, try adding all the bells and whistles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "Num Inputs: 55000\n",
      "Num classes: 10\n",
      "Batch size: 100\n",
      "Epoch: 0001 cost=0.690056207 acc=0.795800001\n",
      "Epoch: 0002 cost=0.261716428 acc=0.924709093\n",
      "Epoch: 0003 cost=0.196611674 acc=0.944400003\n",
      "Epoch: 0004 cost=0.160751927 acc=0.954672732\n",
      "Epoch: 0005 cost=0.136119253 acc=0.962036369\n",
      "Epoch: 0006 cost=0.117553622 acc=0.967600006\n",
      "Epoch: 0007 cost=0.102834631 acc=0.972090917\n",
      "Epoch: 0008 cost=0.090684658 acc=0.975400009\n",
      "Epoch: 0009 cost=0.080365193 acc=0.978181828\n",
      "Epoch: 0010 cost=0.071489379 acc=0.980909102\n",
      "Epoch: 0011 cost=0.063780992 acc=0.983236375\n",
      "Epoch: 0012 cost=0.057070995 acc=0.985200011\n",
      "Epoch: 0013 cost=0.051134932 acc=0.987054556\n",
      "Epoch: 0014 cost=0.045841789 acc=0.988763645\n",
      "Epoch: 0015 cost=0.041127240 acc=0.990309099\n",
      "Epoch: 0016 cost=0.036892837 acc=0.991690917\n",
      "Epoch: 0017 cost=0.033102133 acc=0.992963643\n",
      "Epoch: 0018 cost=0.029688803 acc=0.993963642\n",
      "Epoch: 0019 cost=0.026614433 acc=0.994872732\n",
      "Epoch: 0020 cost=0.023807538 acc=0.995618186\n",
      "Epoch: 0021 cost=0.021292229 acc=0.996381822\n",
      "Epoch: 0022 cost=0.018997483 acc=0.997363639\n",
      "Epoch: 0023 cost=0.016942271 acc=0.998018184\n",
      "Epoch: 0024 cost=0.015052039 acc=0.998454547\n",
      "Epoch: 0025 cost=0.013374861 acc=0.998690910\n",
      "Epoch: 0026 cost=0.011826484 acc=0.998909092\n",
      "Epoch: 0027 cost=0.010438545 acc=0.999127274\n",
      "Epoch: 0028 cost=0.009169483 acc=0.999290910\n",
      "Epoch: 0029 cost=0.008043541 acc=0.999436364\n",
      "Epoch: 0030 cost=0.007017490 acc=0.999581819\n",
      "Epoch: 0031 cost=0.006106484 acc=0.999690909\n",
      "Epoch: 0032 cost=0.005288460 acc=0.999709091\n",
      "Epoch: 0033 cost=0.004571060 acc=0.999781818\n",
      "Epoch: 0034 cost=0.003954154 acc=0.999890909\n",
      "Epoch: 0035 cost=0.003404867 acc=0.999927273\n",
      "Epoch: 0036 cost=0.002926741 acc=0.999927273\n",
      "Epoch: 0037 cost=0.002515100 acc=0.999927273\n",
      "Epoch: 0038 cost=0.002148403 acc=0.999981818\n",
      "Epoch: 0039 cost=0.001839106 acc=0.999981818\n",
      "Epoch: 0040 cost=0.001571554 acc=1.000000000\n",
      "Epoch: 0041 cost=0.001337513 acc=1.000000000\n",
      "Epoch: 0042 cost=0.001136382 acc=1.000000000\n",
      "Epoch: 0043 cost=0.000961244 acc=1.000000000\n",
      "Epoch: 0044 cost=0.000813195 acc=1.000000000\n",
      "Epoch: 0045 cost=0.000686238 acc=1.000000000\n",
      "Epoch: 0046 cost=0.000576581 acc=1.000000000\n",
      "Epoch: 0047 cost=0.000485317 acc=1.000000000\n",
      "Epoch: 0048 cost=0.000406974 acc=1.000000000\n",
      "Epoch: 0049 cost=0.000341000 acc=1.000000000\n",
      "Epoch: 0050 cost=0.000284103 acc=1.000000000\n",
      "Epoch: 0051 cost=0.000236831 acc=1.000000000\n",
      "Epoch: 0052 cost=0.000197247 acc=1.000000000\n",
      "Epoch: 0053 cost=0.000163775 acc=1.000000000\n",
      "Epoch: 0054 cost=0.000135657 acc=1.000000000\n",
      "Epoch: 0055 cost=0.000112133 acc=1.000000000\n",
      "Epoch: 0056 cost=0.000092591 acc=1.000000000\n",
      "Epoch: 0057 cost=0.000076246 acc=1.000000000\n",
      "Epoch: 0058 cost=0.000062670 acc=1.000000000\n",
      "Epoch: 0059 cost=0.000051514 acc=1.000000000\n",
      "Epoch: 0060 cost=0.000042166 acc=1.000000000\n",
      "Epoch: 0061 cost=0.000034574 acc=1.000000000\n",
      "Epoch: 0062 cost=0.000028257 acc=1.000000000\n",
      "Epoch: 0063 cost=0.000023045 acc=1.000000000\n",
      "Epoch: 0064 cost=0.000018810 acc=1.000000000\n",
      "Epoch: 0065 cost=0.000015257 acc=1.000000000\n",
      "Epoch: 0066 cost=0.000012406 acc=1.000000000\n",
      "Epoch: 0067 cost=0.000010074 acc=1.000000000\n",
      "Epoch: 0068 cost=0.000008152 acc=1.000000000\n",
      "Epoch: 0069 cost=0.000006616 acc=1.000000000\n",
      "Epoch: 0070 cost=0.000005340 acc=1.000000000\n",
      "Epoch: 0071 cost=0.000004330 acc=1.000000000\n",
      "Epoch: 0072 cost=0.000003489 acc=1.000000000\n",
      "Epoch: 0073 cost=0.000002812 acc=1.000000000\n",
      "Epoch: 0074 cost=0.000002255 acc=1.000000000\n",
      "Epoch: 0075 cost=0.000001814 acc=1.000000000\n",
      "Epoch: 0076 cost=0.000001460 acc=1.000000000\n",
      "Epoch: 0077 cost=0.000001170 acc=1.000000000\n",
      "Epoch: 0078 cost=0.000000942 acc=1.000000000\n",
      "Epoch: 0079 cost=0.000000756 acc=1.000000000\n",
      "Epoch: 0080 cost=0.000000606 acc=1.000000000\n",
      "Epoch: 0081 cost=0.000000488 acc=1.000000000\n",
      "Epoch: 0082 cost=0.000000392 acc=1.000000000\n",
      "Epoch: 0083 cost=0.000000315 acc=1.000000000\n",
      "Epoch: 0084 cost=0.000000254 acc=1.000000000\n",
      "Epoch: 0085 cost=0.000000205 acc=1.000000000\n",
      "Epoch: 0086 cost=0.000000165 acc=1.000000000\n",
      "Epoch: 0087 cost=0.000000134 acc=1.000000000\n",
      "Epoch: 0088 cost=0.000000109 acc=1.000000000\n",
      "Epoch: 0089 cost=0.000000089 acc=1.000000000\n",
      "Epoch: 0090 cost=0.000000073 acc=1.000000000\n",
      "Epoch: 0091 cost=0.000000060 acc=1.000000000\n",
      "Epoch: 0092 cost=0.000000050 acc=1.000000000\n",
      "Epoch: 0093 cost=0.000000041 acc=1.000000000\n",
      "Epoch: 0094 cost=0.000000035 acc=1.000000000\n",
      "Epoch: 0095 cost=0.000000030 acc=1.000000000\n",
      "Epoch: 0096 cost=0.000000025 acc=1.000000000\n",
      "Epoch: 0097 cost=0.000000022 acc=1.000000000\n",
      "Epoch: 0098 cost=0.000000019 acc=1.000000000\n",
      "Epoch: 0099 cost=0.000000016 acc=1.000000000\n",
      "Epoch: 0100 cost=0.000000014 acc=1.000000000\n"
     ]
    }
   ],
   "source": [
    "from multilayer_perceptron import MultilayerPerceptron\n",
    "\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "n_inputs = len(mnist.train.images)\n",
    "n_classes = len(mnist.train.labels[0])\n",
    "batch_size = 100\n",
    "\n",
    "print('Num Inputs: {}'.format(n_inputs))\n",
    "print('Num classes: {}'.format(n_classes))\n",
    "print('Batch size: {}'.format(batch_size))\n",
    "\n",
    "mlp = MultilayerPerceptron(n_inputs, n_classes, batch_size)\n",
    "mlp.add_layer(300, activation=tf.nn.relu, name=\"hidden_layer\")\n",
    "mlp.add_layer(100, activation=tf.nn.relu, name=\"hidden_layer_2\")\n",
    "mlp.add_layer(10, name=\"output_layer\")\n",
    "mlp.fit(mnist.train.images, mnist.train.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(y_pred, y_true):\n",
    "    assert np.shape(y_pred) == np.shape(y_true)\n",
    "    \n",
    "    correct_predictions = np.sum(y_pred == y_true)\n",
    "    return correct_predictions / np.shape(y_pred)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.9793\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "model_pred = mlp.predict(mnist.test.images)\n",
    "true_pred = np.argmax(mnist.test.labels, axis=1)\n",
    "\n",
    "accuracy = get_accuracy(model_pred, true_pred)\n",
    "print('Accuracy: ', accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
